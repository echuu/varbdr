\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{hyperref}
\setlength{\parindent}{0pt}
% Definitions of handy macros can go here

\usepackage[style=authoryear]{biblatex}
\addbibresource{sample.bib}
\renewcommand*{\nameyeardelim}{\addcomma\space}


\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\tr}{\intercal}
\newcommand{\eye}{\mathrm{I}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\transpose}[1]{#1^{\intercal}}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

% \ShortHeadings{A Variational Approach for Bayesian Density Regression}{}
\firstpageno{1}

\begin{document}

\title{A Variational Approach for Bayesian Density Regression}

\author{\name Eric Chuu \email ericchuu@stat.tamu.edu \\
       \addr Department of Statistics\\
       Texas A\&M University \\
       College Station, TX 77840, USA}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
In the Bayesian density regression problem, mixture of expert models are often used because of their flexibility in estimating conditional densities. In this paper, we discuss the case when covariate dependent weights are used in the approximating mixture density. Under this framework, however, traditional Bayesian methods results in computational difficulties when the dimension of the covariates is large. In order to remedy this problem and to provide a method for faster inference, we propose using a variational approximation to estimate the conditional density. We also discuss different alternative for approximating quantities that lack a closed form so that a coordinate ascent algorithm is viable.
\end{abstract}

\begin{keywords}
  Bayesian Density Regression, Variational Bayes, Mixture Models
\end{keywords}

\section{Introduction}

In the Bayesian density regression problem, we observe data $\left(y_n, x_n \right)_{n=1}^N$, and the goal is the estimate the conditional density of $y \given x$. A common appraoch for doing this is to model the density using a mixture of gaussians, such as the following,
\begin{equation} \label{eq:general_gm}
	f(y \given x) = \sum_{k} \pi_h \mathcal{N} \left(y \given \mu_k(x), \tau_k^{-1} \right)
\end{equation}

While the representation of the density using predictor-independent weights yields less expensive computation, it often lacks flexibility to make it useful in practice and results in a reliance on have too many mixture components. As a result, there have been many proposed models that consider predictor-dependent weights using a kernel stick-breaking process (\cite{dunsonpark:08}) or logit stick-breaking prior (\cite{durante:17}) to generate the weights. In the former method, the increased flexibility comes at heavy computational cost, and in the later method, the process from which the weights are generated does not allow for intuitive inference on the covariates. In our proposed model, the covariates enter through a logistic link function so that we can naturally perform inference on the coefficients. More specifically, we can model
\begin{equation} \label{eq:covdep_gm}
	 f(y \given x) = \sum_{k}^{K} \pi_k(x) \mathcal{N} \left( y \given \mu_k(x), \tau_k^{-1} \right) 
\end{equation} 
where $\mu_k(x) = \transpose{x} \beta_k$ and $\pi_{k} \propto \exp(\transpose{x} \gamma_k)$. 

\section{Notation and Prior Specification}

 



% Acknowledgements should go at the end, before appendices and references

% \acks{We would like to acknowledge support for this project
% from the National Science Foundation (NSF grant IIS-9988642)
% and the Multidisciplinary Research Program of the Department
% of Defense (MURI N00014-00-1-0637). }

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
%%%%% details for updating q(Z) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ } 
\label{app:q_z}

Taking the expectation with respect to the other variational parameters, we can derive the following variational distribution for $\mathbf{Z}$,
\begin{align*}
	\ln q^{*}(\mathbf{Z}) &= \sum_n \sum_k z_{nk} \Bigg\{  -\frac{1}{2}\ln(2\pi) + \frac{1}{2} E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2] \\ 
	&\qquad \qquad \qquad \quad + x_n^{\tr}E_{q(\boldsymbol\gamma)}[\gamma_k] - E_{q(\boldsymbol\gamma)}\Bigg[\ln \left( \sum_{j} \exp \{ x_n^{\tr} \gamma_j \}\right)\Bigg]\Bigg\} \\
	&= \sum_n \sum_k z_{nk} \ln \rho_{nk}
\end{align*}
where we have defined 

\begin{equation} \label{eq:ln_rho}
\begin{split}
 \ln \rho_{nk} = &-\frac{1}{2}\ln(2\pi) + \frac{1}{2} E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2] \\ 
	& + x_n^{\tr}E_{q(\boldsymbol\gamma)}[\gamma_k] - E_{q(\boldsymbol\gamma)}\Bigg[\ln \left( \sum_{j} \exp \{ x_n^{\tr} \gamma_j \}\right)\Bigg]
\end{split}
\end{equation}


Exponentiating and normalizing, we have
\begin{equation} \label{eq:q_z}
	q^{*}(\mathbf{Z}) = \prod_{n} \prod_{k} r_{nk}^{z_{nk}}, \quad r_{nk} = \frac{\rho_{nk}}{\sum_{j} \rho_{nj}}
\end{equation}

For the discrete distribution $q^{*}(\mathbf{Z})$ given in (\ref{eq:q_z}) above, we have $E[z_{nk}] = r_{nk}$. Note, however, that in order to compute the expectation in closed form, we need an expression for the four expectations involved in the quantity $\ln \rho_{nk}$, as defined in (\ref{eq:ln_rho}). \\

From the results derived in Appendix \ref{app:beta_tau}, we know that $q^{*}(\tau_k) = \mathrm{Ga}(\tau_k \given a_k, b_k)$. We can then compute the following expectation with respect to $q^{*}(\boldsymbol\tau))$.

\begin{equation} \label{eq:exp_lntau}
	E_{q(\boldsymbol\tau)}[ \ln \tau_k ] &= \psi(a_k) - \psi(b_k)
\end{equation}

Again from Appendix \ref{app:beta_tau}, we can then compute the following expectation with respect to $q^{*}(\beta_k, \tau_k)$.
\begin{equation} \label{eq:exp_taubeta}
\begin{split}
	E_{q(\boldsymbol\beta, \boldsymbol\tau)}\big[\tau_k (y_n - x_n^{\tr}\beta_k)^2\big] &= 
	E \bigg[\tau_k \left( y_n - m_k^{\tr} x_n x_n^{\tr} m_k + \mathrm{tr} \left(x_n x_n^{\tr}\left(\tau_k V_k \right)^{-1} \right) - 2y_n x_n^{\tr} m_k \right) \bigg] \\
	&=  \frac{a_k}{b_k} \left(y_n^2 + m_k^{\tr}x_nx_n^{\tr} m_k \right) + \mathrm{tr} \left( x_n x_n^{\tr} V_k^{-1}\right) \\
	&= \frac{a_k}{b_k}(y_n + m_k^{\tr}x_n)^2 + x_n^{\tr} V_k^{-1} x_n
\end{split}
\end{equation}


From the expression derived in (\ref{eq:gamma_params}) of Appendix \ref{app:gamma}, we have $q^{*}(\gamma_k) = \mathcal{N}(\gamma_k \given \mu_k, \mathrm{Q}_k^{-1})$, then we have
\begin{equation} \label{eq:exp_gamma}
	E_{q(\gamma_k)}[\gamma_k] = \mu_k
\end{equation}

Using the bound discussed in Appendix \ref{app:gamma}, equation (\ref{eq:jj_bound}), we can then compute the following expectation with respect to $q^{*}(\boldsymbol\gamma)$.
\begin{equation} \label{eq:exp_lse_gamma}
\begin{split}
	& \mathrm{E}_{q(\boldsymbol\gamma)} \Bigg[ \ln \left( \sum_{j}^K \exp \{ x_n^{\tr} \gamma_j \}\right) \Bigg] \\
	& \approx \mathrm{E}_{q(\boldsymbol\gamma)} \Bigg[ \alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n + \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right) \Bigg] \\
	& = \alpha_n + \sum_{j}^K \frac{1}{2}\left(x_n^{\tr}\mu_j - \alpha_n + \xi_{nj}\right) + \lambda(\xi_{nj}) \left( (x_n^{\tr} \mu_j - \alpha_k)^2 - \xi_{nj}^2 + x_j^{\tr} \mathrm{Q}_k^{-1} x_j \right) + \log( 1 + e^{\xi_{nj}})
\end{split}
\end{equation}

Gathering the results in (\ref{eq:exp_lntau}), (\ref{eq:exp_taubeta}), (\ref{eq:exp_gamma}), and (\ref{eq:exp_lse_gamma}), and substituting these into (\ref{eq:ln_rho}), we can compute $E[z_{nk}] = r_{nk}$ in closed form. 


% \newpage

%%%%% details for updating q(Z) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ } \label{app:gamma}  
% contains details for using the Bouchard bound

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

For the variational distribution for $\gamma_k, k = 1, \ldots, K$, we first note the following bound given by~\cite{bouchard:07}, $\sum_{j = 1}^{K} e^{t_j} \leq \prod_{j = 1}^K (1 + e^{t_j})$. Setting $t_j = x_n^{^\intercal} \gamma_j - \alpha_{n}$ and then taking log, we have the following bound:
\begin{equation} \label{eq:log-sum-exp}
	\log \left( \sum_{j = 1}^K \exp\{ x_n^{^\intercal} \gamma_j \}\right) \leq \alpha_n + \sum_{j=1}^K \log \left( 1 + \exp \{ x_n^{^\intercal} \gamma_j - \alpha_n \}\right)
\end{equation}


If we then use the bound from~\cite{jj:2001}, $$\log(1 + e^x) \leq \frac{x - t}{2} + \frac{1}{4t} \tanh \left( \frac{t}{2} \right) (x^2 - t^2) + \log\left(1 + e^t\right)$$ then we arrive at the following bound: 
\begin{equation} \label{eq:jj_bound}
	\log \left( \sum_{j = 1}^K \exp\{ x_n^{^\intercal} \gamma_j \}\right) \leq 
\alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n + \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right)
\end{equation}

where $\lambda(\xi) = \frac{1}{4\xi} \tanh \left( \frac{\xi}{2} \right)$. Then we can substitute this back into $\ln q^{*}(\gamma_k)$ to obtain an approximation for the left hand side of (\ref{eq:log-sum-exp}), thus allowing us to obtain a closed form for the variational distribution. Note that all of the equalities above are written up to constants.
\begin{align*}
    \ln q^{*}(\gamma_k) &= - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \sum_{n} r_{nk} x_n^{\intercal} \gamma_k  - \sum_n r_{nk} \ln \left( \sum_j \exp\{x_n^{\intercal} \gamma_j \} \right)  \\
    & \approx - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \gamma_k^{\intercal} \sum_{n} r_{nk} x_n \\
    & - \sum_n r_{nk} \Bigg\{ \alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n + \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n^2)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right) \Bigg\} \\
    & = - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \gamma_k^{\intercal} \sum_{n} r_{nk} x_n - \sum_n r_{nk} \Bigg\{ \frac{1}{2} \gamma_k^{\tr} x_n + \lambda\left( \xi_{nj} \right) \left( \gamma_j^{\tr}x_n x_n^{\tr} \gamma_j - 2\alpha_n \gamma_j^{\tr} x_n \right)\Bigg\} \\
    &= -\frac{1}{2} \gamma_k^{\tr} \left(\eye_D  + 2 \sum_n r_{nk} \lambda(\xi_{nk}) x_n x_n^{\tr} \right) \gamma_k + \gamma_k' \left( \sum_n r_{nk} \left(\frac{1}{2} + 2 \lambda \left( \xi_{nk}\right) \alpha_n  x_n\right) \right)
\end{align*}
Exponentiating, we can recover $q^{*}(\gamma_k) = \mathcal{N} \left( \mu_k, \mathrm{Q}_k^{-1} \right)$, where
\begin{equation} \label{eq:gamma_params}
\begin{split}
	& \mu_k = \mathrm{Q}_k^{-1} \eta_k \\
	& \eta_k = \sum_{n} r_{nk} \left( \frac{1}{2} + 2 \lambda(\xi_{nj}) \alpha_n \right) x_n \\
	& \mathrm{Q}_k = \eye_D + 2 \sum_{n} r_{nk} \lambda(\xi_{nk}) x_n x_n^{\tr}
\end{split}
\end{equation}

The additional parameters introduced in the two upper bounds can be updated using the following equations
\begin{align*}
    \xi_{nk} & = \sqrt{\left(\mu_k^{\intercal}x_n - \alpha_n \right)^2 + x_n^{\intercal} \mathrm{Q}_k^{-1} x_n} \qquad \qquad \forall k, n \\ \\
    \alpha_n & = \frac{\frac{1}{2}\left( \frac{K}{2} - 1\right) + \sum_{j = 1}^K \lambda \left( \xi_{nj} \right)\mu_j^{\intercal} x_n}{\sum_{j=1}^{K} \lambda \left( \xi_{nj}\right)} \qquad \forall n
\end{align*}


%%%%% details for updating q(beta, tau) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ } \label{app:beta_tau}  
Using results from Appendix \ref{app:q_z}, we can write the following expression for the joint variational distribution of $(\beta_k, \tau_k)$, 
\begin{equation} \label{eq:lnq_beta_tau}
\begin{split}
	\ln q^{*}(\beta_k, \tau_k) = & \sum_{n} -\frac{1}{2} r_{nk} \tau_k \left( y_n^2 + \transpose{\beta_k} x_n \transpose{x_n} \beta_k - 2y_n \transpose{\beta_k} x_n \right) + \frac{r_{nk}}{2} \ln \tau_k + \frac{D}{2} \ln \tau_k \\
	& - \frac{\tau_k}{2} \left( \transpose{\beta_k} \Lambda_0 \beta_k + \transpose{m_0} \Lambda_0 m_0 - 2\transpose{\beta_k}\Lambda_0m_0\right) + (a_0 - 1) \ln \tau_k - b_0 \tau_k
\end{split}
\end{equation}

We first consider terms on the right hand side of (\ref{eq:lnq_beta_tau}) that depend on $beta_k$ to find $\ln q^{\star}(\beta_k \given \tau_k)$, giving
\begin{equation} \label{eq:lnq_beta}
	\ln q^{\star}(\beta_k \given \tau_k) = -\frac{\tau_k}{2} \transpose{\beta_k} \Big[ \sum_{n}r_{nk} x_n \transpose{x_n} + \Lambda_0 \Big] \beta_k + \tau_k \transpose{\beta_k} \Big[ \sum_{n} r_{nk}y_n x_n + \Lambda_0 m_0 \Big]
\end{equation}
\begin{equation} \label{q_beta}
	& q^{*}(\beta_k \given \tau_k) = \mathcal{N}\left(m_k, (\tau_k \mathrm{V}_k)^{-1} \right)
\end{equation}
\begin{equation} \label{eq:beta_params}
\begin{split}
	& m_k = \mathrm{V}_k^{-1}  b_k \\
 	& \mathrm{V}_k = \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 \\
 	& b_k = \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0
\end{split}
\end{equation}

Then we can make use of the relation $\ln q^{*}(\tau_k) = \ln q^{\star}(\beta_k, \tau_k) -  \ln q^{\star}(\beta_k \given \tau_k)$, where the quantities on the right hand side come from (\ref{eq:lnq_beta_tau}) and (\ref{q_beta}). Note that equality below is written up to constants, keeping only terms involving $\tau_k$. 
\begin{equation}\label{eq:ln_q_tau}
\begin{split}
	 \ln q^{*}(\tau_k) = (a_0 + N_k - 1) \ln \tau_k - \tau_k \Bigg\{ b_0 & + \frac{1}{2} \left( \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{m_k} \mathrm{V}_k m_k \right) \\
	 & + \frac{1}{2} \transpose{\beta_k} \left( \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 - \mathrm{V}_k \right) \beta_k \\
	 & - 2 \transpose{\beta_k} \left( \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0 - \mathrm{V}_k m_k \right) \Bigg\}
\end{split}
\end{equation}
Exponentiating, we arrive at the following distribution
\begin{equation} \label{eq:q_tau}
	q^{*}(\tau_k) =  \mathrm{Ga}\left( \tau_k \given a_k, b_k \right)
\end{equation}
where we have defined
\begin{equation} \label{eq:tau_params}
\begin{split}
	& a_k = a_0 + N_k \\
	& b_k = b_0 + \frac{1}{2} \sum_{n} r_{nk} y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{b_k} \mathrm{V}_k^{-1} b_k
\end{split}
\end{equation}

The expression for $b_k$ arises by noting that the three following simplifications for the summation terms in the coefficient of $\tau_k$ in (\ref{eq:ln_q_tau}), 
\begin{align*}
	& \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{m_k} \mathrm{V}_k m_k =  \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 -\transpose{b_k} \mathrm{V}_k^{-1} b_k \\
	& \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 - \mathrm{V}_k = 0 \\
	& \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0 - \mathrm{V}_k m_k = 0
\end{align*}

where the first equality holds by expanding $\transpose{m_k} \mathrm{V}_k m_k = \transpose{b_k} \left( \mathrm{V}_k^{-1}\right)^{\intercal} \mathrm{V}_k \mathrm{V}_k^{-1} b_k = \transpose{b_k} \mathrm{V}_k^{-1} b_k$. The second quality holds by recalling the definition of $\mathrm{V}_k$ in (\ref{eq:beta_params}), and the third equality holds by observing from (\ref{eq:beta_params}) that $\mathrm{V}_{k} m_k = b_k$

\section{ } %% EBLO DERIVATION




\vskip 0.2in
\bibliography{sample}

\end{document}