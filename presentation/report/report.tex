\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{hyperref}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\setlength{\parindent}{0pt}
% Definitions of handy macros can go here

\usepackage[style=authoryear]{biblatex}
\addbibresource{sample.bib}
\renewcommand*{\nameyeardelim}{\addcomma\space}


\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\tr}{\intercal}
\newcommand{\eye}{\mathrm{I}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\transpose}[1]{#1^{\intercal}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\nprod}{\prod_{n}}
\newcommand{\kprod}{\prod_{k}}
\newcommand{\nsum}{\sum_{n}}
\newcommand{\ksum}{\sum_{k}}
\newcommand{\boldbeta}{\boldsymbol\beta}
\newcommand{\boldgamma}{\boldsymbol\gamma}
\newcommand{\boldtau}{\boldsymbol\tau}
\newcommand{\sumexp}{\sum_{j=1}^{K} \exp \{ \transpose{x_n} \gamma_j \}}
\newcommand{\E}{\mathbb{E}}




\newcommand{\pr}[1]{p \left( #1 \right)}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

% \ShortHeadings{A Variational Approach for Bayesian Density Regression}{}
\firstpageno{1}

\begin{document}

\title{A Variational Approach for Bayesian Density Regression}

\author{\name Eric Chuu \email ericchuu@stat.tamu.edu \\
       \addr Department of Statistics\\
       Texas A\&M University \\
       College Station, TX 77840, USA}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
In the Bayesian density regression problem, mixture of expert models are often used because of their flexibility in estimating conditional densities. In this paper, we discuss the case when covariate dependent weights are used in the approximating mixture density. Under this framework, however, traditional Bayesian methods results in computational difficulties when the dimension of the covariates is large. In order to remedy this problem and to provide a method for faster inference, we propose using a variational approximation to estimate the conditional density. We also discuss different alternative for approximating quantities that lack a closed form so that a coordinate ascent algorithm is viable.
\end{abstract}

\begin{keywords}
  Bayesian Density Regression, Variational Bayes, Mixture Models
\end{keywords}

\section{Introduction}

In the Bayesian density regression problem, we observe data $\left(y_n, x_n \right)_{n=1}^N$, and the goal is the estimate the conditional density of $y \given x$. A common appraoch for doing this is to model the density using a mixture of gaussians, such as the following,
\begin{equation} \label{eq:general_gm}
	f(y \given x) = \sum_{k} \pi_k \mathcal{N} \left(y \given \mu_k(x), \tau_k^{-1} \right)
\end{equation}

While the representation of the density using predictor-independent weights yields less expensive computation, it often lacks flexibility to make it useful in practice and results in a reliance on have too many mixture components. As a result, there have been many proposed models that consider predictor-dependent weights using a kernel stick-breaking process (\cite{dunsonpark:08}) or logit stick-breaking prior (\cite{durante:17}) to generate the weights. In the former method, the increased flexibility comes at heavy computational cost, and in the later method, the process from which the weights are generated does not allow for intuitive inference on the covariates. In our proposed model, the covariates enter through a logistic link function so that we can naturally perform inference on the coefficients. More specifically, we can model
\begin{equation} \label{eq:covdep_gm}
	 f(y \given x) = \sum_{k}^{K} \pi_k(x) \mathcal{N} \left( y \given \mu_k(x), \tau_k^{-1} \right) 
\end{equation} 
where $\mu_k(x) = \transpose{x} \beta_k$ and $\pi_{k} \propto \exp(\transpose{x} \gamma_k)$. 

\section{Notation and Prior Specification}
For the set of observed data, we denote $\mathbf{y} = \{y_1, \ldots, y_N \}, \mathbf{X} = \{ x_1, \ldots, x_N \}$, where each $x_n \in \R^{D}$. Let $\boldbeta = \{ \beta_1, \ldots, \beta_K\}$ and $\boldgamma = \{ \gamma_1, \ldots, \gamma_k\}$ denote the $D$-dimensional coefficient vectors  in the mixture weights and in gaussian mixture components, respectively. Finally, let $\boldtau = \{ \tau_1, \ldots, \tau_k \}$ denote the precision parameters. We introduce the set of latent variables $\mathbf{Z} = \{ z_1, \ldots, z_N \}$, where $z_n \in \R^K$ and $z_{nk} = 1$ if $y_n$ belongs to the $k$-th cluster so that $\sum_{k} z_{nk} = 1$. Conditioning on $\mathbf{Z}$, we have the following simplified form of the marginal likelihood. 

%% should probably include a figure displaying the graphical model, similar ot the one shown on p.475 (Bishop)

\begin{equation} \label{eq:simp_lik}
	p \left( \mathbf{y} \given \mathbf{X}, \boldsymbol\beta, \boldsymbol{\tau}, \mathbf{Z}, \boldsymbol\gamma \right) = 
	\prod_{n} \prod_{k} \mathcal{N} \left( y_n \given \transpose{x_n} \beta_k, \tau_{k}^{-1} \right)^{z_{nk}}
\end{equation}
\begin{equation} \label{eq:z_prior}
	p \left( \mathbf{Z} \given \mathbf{X}, \boldsymbol\gamma \right) = \nprod \kprod \pi_{k} (x_n)^{z_{nk}} = 
	\nprod \kprod \left( \frac{\exp\{\transpose{x_n} \gamma_k\}}{\sumexp}\right)^{z_{nk}}
\end{equation}

Next, we introduce the priors over the parameters $\boldbeta, \boldtau, \boldgamma$, where we simplify the calculations by considering conjugate priors. 

\begin{equation} \label{eq:gamma_prior}
	 p(\boldgamma) = \kprod p(\gamma_k) = \mathcal{N} \left( \gamma_k \given 0, \eye_D \right)
\end{equation}

\begin{equation} \label{eq:betatau_prior}
	 p\left( \boldbeta, \boldtau \right) = \prod_{k} p(\beta_k, \tau_k) = \kprod p(\beta_k \given \tau_k) p(\tau_k) = 
	 \kprod \mathcal{N} \left( \beta_k \given m_0, (\tau_k \Lambda_0)^{-1} \right) \mathrm{Ga} \left( \tau_k \given a_0, b_0 \right)
\end{equation}


\section{Variational Distribution}
At this point, the variational parameters of interest are $\boldsymbol \theta = (\mathbf{Z}, \boldbeta, \boldtau, \boldgamma)$. The log of the joint distibution of these random variables is given by
\begin{equation} \label{eq:joint}
\begin{split}
	\ln \bigg\{ \pr{\mathbf{y}, \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}} \bigg\} &= 
	\ln \bigg\{\pr{\mathbf{y} \given \mathbf{X}, \boldsymbol\beta, \boldsymbol{\tau}, \mathbf{Z}, \boldsymbol\gamma}\pr{\mathbf{Z}\given \mathbf{X}, \boldgamma}\pr{\boldgamma}\pr{\boldbeta, \boldtau} \bigg\}\\
	&= \nsum \ksum z_{nk}\bigg\{ -\frac{1}{2}\ln(2\pi) + \frac{1}{2} \ln \tau_k - \frac{\tau_k}{2} \left( y_n - \transpose{x_n}\beta_k\right)^2 \bigg\} \\
	& + \nsum \ksum z_{nk} \bigg\{ \transpose{x_n} \gamma_k - \ln\left( \sumexp \right) \bigg\} \\
	& + \ksum  \bigg\{ -\frac{D}{2} \ln (2\pi) + \frac{D}{2} \ln \tau_k + \ln | \Lambda_0| - \frac{\tau_k}{2}\transpose{(\beta_k - m_0)} \Lambda_0 (\beta_k - m_0)\bigg\} \\
	& + \ksum \bigg\{ (a_0 - 1) \ln \tau_k - b_0 \tau_k \bigg\}
\end{split}
\end{equation}

We consider the following variational distribution used to approximate the posterior distribution of the parameters outlined previously.
\begin{equation} \label{eq:variational}
	q \left( \mathbf{Z}, \boldbeta, \boldtau, \boldgamma \right) = q(\mathbf{Z}) q(\boldbeta, \boldtau, \boldgamma)
\end{equation}

\subsection{Coordinate Ascent Updates}

As is standard in variational algorithms, we now seek the sequential updates of the factors in (\ref{eq:variational}). Taking the expectation with respect to the other variational parameters, we can derive the following update equation for $q(\mathbf{Z})$,
\begin{equation} \label{eq:ln_z_update}
	\ln q^{*}(\mathbf{Z}) = E_{-q(\mathbf{Z})} \Big[ \ln \big\{ \pr{\mathbf{y}, \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}, \boldgamma} \big\} \Big]
\end{equation}
We adopt the convention that the expectation with respect to a negative subscript indicates an expectation taken with repect to the other variational parameters. Ignoring terms that are not functionally dependent on $\mathbf{Z}$, we can exponentiate both sides of (\ref{eq:ln_z_update}) to obtain the optimal solution for $q(\mathbf{Z})$,
\begin{equation} \label{optimal_z}
	 q^{*}(\mathbf{Z}) = \prod_{n} \prod_{k} r_{nk}^{z_{nk}}, \quad r_{nk} = \frac{\rho_{nk}}{\sum_{j} \rho_{nj}}
\end{equation}
See Appendix \ref{app:q_z} for the details in this calculation. % More needs to be said here about how rho_nk depends on the variational distribution of the other parameters; for now, we just write down the general outline of the updates and aim to fill in details later

It remains to consider the factor $q( \boldbeta, \boldtau, \boldgamma )$. Taking the expectation with respect to $q(\mathbf{Z})$, we have the following equality written up to constants,
\begin{equation} \label{eq:ln_beta_tau_gamma}
\begin{split}
	\ln q^{*}(\boldbeta, \boldtau, \boldgamma) &=  \ksum \nsum  E_{q(\mathbf{Z})}[z_{nk}] \ln \mathcal{N} \left( y_n \given \transpose{x_n} \beta_k, \tau_{k}^{-1} \right) + \ksum \ln \pr{\beta_k, \tau_k} \\
	& + \ksum \nsum E_{q(\mathbf{Z})} \left( \transpose{x_n} \gamma_k - \ln \left( \sumexp\right)\right) + \ksum \ln \mathcal{N}(\gamma_k \given 0, \eye_D)
\end{split}
\end{equation}

From the expression above, we see that the optimal distribution has a sum involving only the $\gamma_k$'s and a sum involving only the $(\beta_k, \tau_k)$'s, which implies $q(\boldbeta, \boldtau, \boldgamma) = \big[ \kprod q(\beta_k, \tau_k) \big] \kprod q(\gamma_k)$.

With this factorization in mind, we can obtain the following updates for the remaining variational parameters,
\begin{equation} \label{optimal_beta_given_tau}
	q^{*}(\beta_k \given \tau_k) = \mathcal{N}\left(\beta_k \given m_k, (\tau_k \mathrm{V}_k)^{-1} \right)
\end{equation}

\begin{equation} \label{optimal_tau}
	q^{*}(\tau_k) =  \mathrm{Ga}\left( \tau_k \given a_k, b_k \right)
\end{equation}

\begin{equation} \label{optimal_gamma}
	q^{*}(\gamma_k) = \mathcal{N} \left( \gamma_k \given \mu_k, \mathrm{Q}_k^{-1} \right)
\end{equation}

for $k = 1, \ldots, K$. The derivation for (\ref{optimal_beta_given_tau}) and (\ref{optimal_tau}) can be found in Appendix \ref{app:beta_tau}, and the details for (\ref{optimal_gamma}) can be found in Appendix \ref{app:gamma}.

% decide if we want to write out definitions for m_k, V_k, a_k, b_k, mu_k, Q_k here for clarity's sake 
% even though we already include the definitions in the corresponding appendix


\subsection{Evidence Lower Bound (ELBO)}
In order to evaluate the convergence of the algorithm, we can calculate the evidence lower bound after updating all the variational parameters. Since the ELBO is monotonic increasing, we continue the coordinate ascent until the change in the ELBO falls below a predetermined tolerance. Note that the expectations taken below are with respect to the optimal variational distributions derived in the previous section.

\begin{equation} \label{eq:ELBO}
\begin{split}
	\mathcal{L}(q) &= \sum_{z} \int \int \int q(\boldbeta, \boldtau, \boldgamma, \mathbf{Z})
	\ln \Big\{ \frac{p(\mathbf{y}, \mathbf{X}, \boldbeta, \boldtau, \boldgamma, \mathbf{Z})}{q(\boldbeta, \boldtau, \boldgamma, \mathbf{Z})}\Big\} d\boldbeta d\boldtau d\boldgamma \\
	&= \E[\ln\pr{\mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}}] + 
	\E[\ln \pr{\mathbf{Z} \given \mathbf{X}, \boldgamma}] + \E[\ln \pr{\boldgamma}] + \E[\ln \pr{\boldbeta, \boldtau}] \\
	& \quad - \E[\ln q(\mathbf{Z})] - \E[\ln q(\boldbeta, \boldtau)] - \E[\ln q(\boldgamma)]
\end{split}
\end{equation}

Each of the seven expectations can be expressed as follows. Details for each calculation can be found in Appendix \ref{app:elbo}. 
\begin{align*}
	& \E[\ln\pr{\mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}}] = -\frac{1}{2}\nsum \ksum r_{nk} \Big\{ \left( \ln (2\pi) - \psi(a_k) + \psi(b_k)\right)  + \frac{a_k}{b_k}(y_n - \transpose{x_n}m_k)^2 + \transpose{x_n}\mathrm{V}_k^{-1}x_n \Big\} \\
	 & \E[\ln \pr{\mathbf{Z} \given \mathbf{X}, \boldgamma}] = \nsum \ksum r_{nk} \left( \transpose{x_n}\mu_k - \alpha_n - \varphi_n \right) \\
	 & \E[\ln \pr{\boldgamma}] = \frac{K \cdot D}{2} \ln (2\pi) - \frac{1}{2} \ksum \transpose{\mu_k}\mu_k \\
	 & \E[\ln \pr{\boldbeta, \boldtau}] = \ksum -\frac{D}{2}\ln(2\pi) + \left( \frac{D}{2} + a_0 - 1 \right) (\psi(a_k) - \psi(b_k)) + \ln |\Lambda_0| + \ln \frac{b_0^{a_0}}{\Gamma(a_0)} \\
	& \qquad \qquad \qquad \qquad  -\frac{1}{2} \Big\{ \frac{a_k}{b_k} \transpose{(m_k - m_0)} \Lambda_0 (m_k - m_0) + \mathrm{tr}\left( \Lambda_0 \mathrm{V}_k^{-1}\right)\Big\} \\
	& \E[\ln q(\mathbf{Z})] =  \nsum \ksum r_{nk} \ln r_{nk} \\
	& \E[\ln q(\boldbeta, \boldtau)] = -\frac{D}{2} \ln (2\pi) + \frac{1}{2} \ln |V_k| + C(a_k, b_k) \\
	& \E[\ln q(\boldgamma)] = \sum_{k} \ln|\mathrm{Q}_k| - \frac{D}{2}(\ln (2\pi) + 1)
\end{align*}


\section{Algorithm}

Using the updates discussed in the previous section, we can formalize the variational algorithm below.

\begin{algorithm}
\caption{Variational Approximation for Gaussian Mixture}\label{alg:vb}
\begin{algorithmic}[1]
\Procedure{MyProcedure}{}
\State $\textit{stringlen} \gets \text{length of }\textit{string}$
\State $i \gets \textit{patlen}$
\BState \emph{top}:
\If {$i > \textit{stringlen}$} \Return false
\EndIf
\State $j \gets \textit{patlen}$
\BState \emph{loop}:
\If {$\textit{string}(i) = \textit{path}(j)$}
\State $j \gets j-1$.
\State $i \gets i-1$.
\State \textbf{goto} \emph{loop}.
\State \textbf{close};
\EndIf
\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
\State \textbf{goto} \emph{top}.
\EndProcedure
\end{algorithmic}
\end{algorithm}



% Acknowledgements should go at the end, before appendices and references

\newpage

\appendix
%%%%% details for updating q(Z) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ } 
\label{app:q_z}

Taking the expectation with respect to the other variational parameters, we can derive the following variational distribution for $\mathbf{Z}$,
\begin{align*}
	\ln q^{*}(\mathbf{Z}) &= \sum_n \sum_k z_{nk} \Bigg\{  -\frac{1}{2}\ln(2\pi) + \frac{1}{2} E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2] \\ 
	&\qquad \qquad \qquad \quad + x_n^{\tr}E_{q(\boldsymbol\gamma)}[\gamma_k] - E_{q(\boldsymbol\gamma)}\Bigg[\ln \left( \sum_{j} \exp \{ x_n^{\tr} \gamma_j \}\right)\Bigg]\Bigg\} \\
	&= \sum_n \sum_k z_{nk} \ln \rho_{nk}
\end{align*}
where we have defined 

\begin{equation} \label{eq:ln_rho}
\begin{split}
 \ln \rho_{nk} = &-\frac{1}{2}\ln(2\pi) + \frac{1}{2} E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2] \\ 
	& + x_n^{\tr}E_{q(\boldsymbol\gamma)}[\gamma_k] - E_{q(\boldsymbol\gamma)}\Bigg[\ln \left( \sum_{j} \exp \{ x_n^{\tr} \gamma_j \}\right)\Bigg]
\end{split}
\end{equation}


Exponentiating and normalizing, we have
\begin{equation} \label{eq:q_z}
	q^{*}(\mathbf{Z}) = \prod_{n} \prod_{k} r_{nk}^{z_{nk}}, \quad r_{nk} = \frac{\rho_{nk}}{\sum_{j} \rho_{nj}}
\end{equation}

For the discrete distribution $q^{*}(\mathbf{Z})$ given in (\ref{eq:q_z}) above, we have $E[z_{nk}] = r_{nk}$. Note, however, that in order to compute the expectation in closed form, we need an expression for the four expectations involved in the quantity $\ln \rho_{nk}$, as defined in (\ref{eq:ln_rho}). \\

From the results derived in Appendix \ref{app:beta_tau}, we know that $q^{*}(\tau_k) = \mathrm{Ga}(\tau_k \given a_k, b_k)$. We can then compute the following expectation with respect to $q^{*}(\boldsymbol\tau))$.

\begin{equation} \label{eq:exp_lntau}
	E_{q(\boldsymbol\tau)}[ \ln \tau_k ] &= \psi(a_k) - \psi(b_k)
\end{equation}

Again from Appendix \ref{app:beta_tau}, we can then compute the following expectation with respect to $q^{*}(\beta_k, \tau_k)$.
\begin{equation} \label{eq:exp_taubeta}
\begin{split}
	E_{q(\boldsymbol\beta, \boldsymbol\tau)}\big[\tau_k (y_n - x_n^{\tr}\beta_k)^2\big] &= 
	E \bigg[\tau_k \left( y_n - m_k^{\tr} x_n x_n^{\tr} m_k + \mathrm{tr} \left(x_n x_n^{\tr}\left(\tau_k V_k \right)^{-1} \right) - 2y_n x_n^{\tr} m_k \right) \bigg] \\
	&=  \frac{a_k}{b_k} \left(y_n^2 + m_k^{\tr}x_nx_n^{\tr} m_k \right) + \mathrm{tr} \left( x_n x_n^{\tr} V_k^{-1}\right) \\
	&= \frac{a_k}{b_k}(y_n + m_k^{\tr}x_n)^2 + x_n^{\tr} V_k^{-1} x_n
\end{split}
\end{equation}


From the expression derived in (\ref{eq:gamma_params}) of Appendix \ref{app:gamma}, we have $q^{*}(\gamma_k) = \mathcal{N}(\gamma_k \given \mu_k, \mathrm{Q}_k^{-1})$, then we have
\begin{equation} \label{eq:exp_gamma}
	E_{q(\gamma_k)}[\gamma_k] = \mu_k
\end{equation}

Using the bound discussed in Appendix \ref{app:gamma}, equation (\ref{eq:jj_bound}), we can then compute the following expectation with respect to $q^{*}(\boldsymbol\gamma)$.
\begin{equation} \label{eq:exp_lse_gamma}
\begin{split}
	& \mathrm{E}_{q(\boldsymbol\gamma)} \Bigg[ \ln \left( \sum_{j}^K \exp \{ x_n^{\tr} \gamma_j \}\right) \Bigg] \\
	& \approx \mathrm{E}_{q(\boldsymbol\gamma)} \Bigg[ \alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n + \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right) \Bigg] \\
	& = \alpha_n + \sum_{j}^K \frac{1}{2}\left(x_n^{\tr}\mu_j - \alpha_n + \xi_{nj}\right) + \lambda(\xi_{nj}) \left( (x_n^{\tr} \mu_j - \alpha_k)^2 - \xi_{nj}^2 + x_j^{\tr} \mathrm{Q}_k^{-1} x_j \right) + \log( 1 + e^{\xi_{nj}})
\end{split}
\end{equation}

Gathering the results in (\ref{eq:exp_lntau}), (\ref{eq:exp_taubeta}), (\ref{eq:exp_gamma}), and (\ref{eq:exp_lse_gamma}), and substituting these into (\ref{eq:ln_rho}), we can compute $E[z_{nk}] = r_{nk}$ in closed form. 


% \newpage

%%%%% details for updating q(Z) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ } \label{app:gamma}  
% contains details for using the Bouchard bound

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

For the variational distribution for $\gamma_k, k = 1, \ldots, K$, we first note the following bound given by~\cite{bouchard:07}, $\sum_{j = 1}^{K} e^{t_j} \leq \prod_{j = 1}^K (1 + e^{t_j})$. Setting $t_j = x_n^{^\intercal} \gamma_j - \alpha_{n}$ and then taking log, we have the following bound:
\begin{equation} \label{eq:log-sum-exp}
	\log \left( \sum_{j = 1}^K \exp\{ x_n^{^\intercal} \gamma_j \}\right) \leq \alpha_n + \sum_{j=1}^K \log \left( 1 + \exp \{ x_n^{^\intercal} \gamma_j - \alpha_n \}\right)
\end{equation}


If we use the bound from~\cite{jj:2001}, $$\log(1 + e^x) \leq \frac{x - t}{2} + \frac{1}{4t} \tanh \left( \frac{t}{2} \right) (x^2 - t^2) + \log\left(1 + e^t\right)$$ then we arrive at the following bound: 
\begin{equation} \label{eq:jj_bound}
	\log \left( \sum_{j = 1}^K \exp\{ x_n^{^\intercal} \gamma_j \}\right) \leq 
\alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n + \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right)
\end{equation}

where $\lambda(\xi) = \frac{1}{4\xi} \tanh \left( \frac{\xi}{2} \right)$. Then we can substitute this back into $\ln q^{*}(\gamma_k)$ to obtain an approximation for the left hand side of (\ref{eq:log-sum-exp}), thus allowing us to obtain a closed form for the variational distribution. Note that all of the equalities above are written up to constants.
\begin{align*}
    \ln q^{*}(\gamma_k) &= - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \sum_{n} r_{nk} x_n^{\intercal} \gamma_k  - \sum_n r_{nk} \ln \left( \sum_j \exp\{x_n^{\intercal} \gamma_j \} \right)  \\
    & \approx - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \gamma_k^{\intercal} \sum_{n} r_{nk} x_n \\
    & - \sum_n r_{nk} \Bigg\{ \alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n + \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n^2)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right) \Bigg\} \\
    & = - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \gamma_k^{\intercal} \sum_{n} r_{nk} x_n - \sum_n r_{nk} \Bigg\{ \frac{1}{2} \gamma_k^{\tr} x_n + \lambda\left( \xi_{nj} \right) \left( \gamma_j^{\tr}x_n x_n^{\tr} \gamma_j - 2\alpha_n \gamma_j^{\tr} x_n \right)\Bigg\} \\
    &= -\frac{1}{2} \gamma_k^{\tr} \left(\eye_D  + 2 \sum_n r_{nk} \lambda(\xi_{nk}) x_n x_n^{\tr} \right) \gamma_k + \gamma_k' \left( \sum_n r_{nk} \left(\frac{1}{2} + 2 \lambda \left( \xi_{nk}\right) \alpha_n  x_n\right) \right)
\end{align*}
Exponentiating, we can recover $q^{*}(\gamma_k) = \mathcal{N} \left(\gamma_k \given \mu_k, \mathrm{Q}_k^{-1} \right)$, where
\begin{equation} \label{eq:gamma_params}
\begin{split}
	& \mu_k = \mathrm{Q}_k^{-1} \eta_k \\
	& \eta_k = \sum_{n} r_{nk} \left( \frac{1}{2} + 2 \lambda(\xi_{nj}) \alpha_n \right) x_n \\
	& \mathrm{Q}_k = \eye_D + 2 \sum_{n} r_{nk} \lambda(\xi_{nk}) x_n x_n^{\tr}
\end{split}
\end{equation}

The additional parameters introduced in the two upper bounds can be updated using the following equations
\begin{align*}
    \xi_{nk} & = \sqrt{\left(\mu_k^{\intercal}x_n - \alpha_n \right)^2 + x_n^{\intercal} \mathrm{Q}_k^{-1} x_n} \qquad \qquad \forall k, n \\ \\
    \alpha_n & = \frac{\frac{1}{2}\left( \frac{K}{2} - 1\right) + \sum_{j = 1}^K \lambda \left( \xi_{nj} \right)\mu_j^{\intercal} x_n}{\sum_{j=1}^{K} \lambda \left( \xi_{nj}\right)} \qquad \forall n
\end{align*}


%%%%% details for updating q(beta, tau) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ } \label{app:beta_tau}  
Using results from Appendix \ref{app:q_z}, we can write the following expression for the joint variational distribution of $(\beta_k, \tau_k)$, 
\begin{equation} \label{eq:lnq_beta_tau}
\begin{split}
	\ln q^{*}(\beta_k, \tau_k) = & \sum_{n} -\frac{1}{2} r_{nk} \tau_k \left( y_n^2 + \transpose{\beta_k} x_n \transpose{x_n} \beta_k - 2y_n \transpose{\beta_k} x_n \right) + \frac{r_{nk}}{2} \ln \tau_k + \frac{D}{2} \ln \tau_k \\
	& - \frac{\tau_k}{2} \left( \transpose{\beta_k} \Lambda_0 \beta_k + \transpose{m_0} \Lambda_0 m_0 - 2\transpose{\beta_k}\Lambda_0m_0\right) + (a_0 - 1) \ln \tau_k - b_0 \tau_k
\end{split}
\end{equation}

We first consider terms on the right hand side of (\ref{eq:lnq_beta_tau}) that depend on $beta_k$ to find $\ln q^{\star}(\beta_k \given \tau_k)$, giving
\begin{equation} \label{eq:lnq_beta}
	\ln q^{\star}(\beta_k \given \tau_k) = -\frac{\tau_k}{2} \transpose{\beta_k} \Big[ \sum_{n}r_{nk} x_n \transpose{x_n} + \Lambda_0 \Big] \beta_k + \tau_k \transpose{\beta_k} \Big[ \sum_{n} r_{nk}y_n x_n + \Lambda_0 m_0 \Big]
\end{equation}
\begin{equation} \label{q_beta}
	q^{*}(\beta_k \given \tau_k) = \mathcal{N}\left(\beta_k \given m_k, (\tau_k \mathrm{V}_k)^{-1} \right)
\end{equation}
\begin{equation} \label{eq:beta_params}
\begin{split}
	& m_k = \mathrm{V}_k^{-1}  b_k \\
 	& \mathrm{V}_k = \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 \\
 	& b_k = \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0
\end{split}
\end{equation}

Then we can make use of the relation $\ln q^{*}(\tau_k) = \ln q^{\star}(\beta_k, \tau_k) -  \ln q^{\star}(\beta_k \given \tau_k)$, where the quantities on the right hand side come from (\ref{eq:lnq_beta_tau}) and (\ref{q_beta}). Note that equality below is written up to constants, keeping only terms involving $\tau_k$. 
\begin{equation}\label{eq:ln_q_tau}
\begin{split}
	 \ln q^{*}(\tau_k) = (a_0 + N_k - 1) \ln \tau_k - \tau_k \Bigg\{ b_0 & + \frac{1}{2} \left( \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{m_k} \mathrm{V}_k m_k \right) \\
	 & + \frac{1}{2} \transpose{\beta_k} \left( \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 - \mathrm{V}_k \right) \beta_k \\
	 & - 2 \transpose{\beta_k} \left( \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0 - \mathrm{V}_k m_k \right) \Bigg\}
\end{split}
\end{equation}
Exponentiating, we arrive at the following distribution
\begin{equation} \label{eq:q_tau}
	q^{*}(\tau_k) =  \mathrm{Ga}\left( \tau_k \given a_k, b_k \right)
\end{equation}
where we have defined
\begin{equation} \label{eq:tau_params}
\begin{split}
	& a_k = a_0 + N_k \\
	& b_k = b_0 + \frac{1}{2} \sum_{n} r_{nk} y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{b_k} \mathrm{V}_k^{-1} b_k
\end{split}
\end{equation}

The expression for $b_k$ arises by noting that the three following simplifications for the summation terms in the coefficient of $\tau_k$ in (\ref{eq:ln_q_tau}), 
\begin{align*}
	& \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{m_k} \mathrm{V}_k m_k =  \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 -\transpose{b_k} \mathrm{V}_k^{-1} b_k \\
	& \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 - \mathrm{V}_k = 0 \\
	& \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0 - \mathrm{V}_k m_k = 0
\end{align*}

where the first equality holds by expanding $\transpose{m_k} \mathrm{V}_k m_k = \transpose{b_k} \left( \mathrm{V}_k^{-1}\right)^{\intercal} \mathrm{V}_k \mathrm{V}_k^{-1} b_k = \transpose{b_k} \mathrm{V}_k^{-1} b_k$. The second quality holds by recalling the definition of $\mathrm{V}_k$ in (\ref{eq:beta_params}), and the third equality holds by observing from (\ref{eq:beta_params}) that $\mathrm{V}_{k} m_k = b_k$

\section{ } \label{app:elbo}  %% EBLO DERIVATION

\begin{equation} \label{eq:e1_deriv}
\begin{split}
	\E[\ln\pr{\mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}}] &= 
\end{split}
\end{equation}

\begin{equation} \label{eq:e2_deriv}
\begin{split}
	\E[\ln \pr{\mathbf{Z} \given \mathbf{X}, \boldgamma}] &=
\end{split}
\end{equation}

\begin{equation} \label{eq:e3_deriv}
\begin{split}
	\E[\ln \pr{\boldgamma}] &= \ksum \frac{D}{2} \ln (2\pi) - \frac{1}{2} \transpose{\mu_k}\mu_k
\end{split}
\end{equation}

\begin{equation} \label{eq:e4_deriv}
\begin{split}
	\E[\ln \pr{\boldbeta, \boldtau}] &= 
\end{split}
\end{equation}

\begin{equation} \label{eq:e5_deriv}
\begin{split}
	\E[\ln q(\mathbf{Z})] &=  \nsum \ksum r_{nk} \ln r_{nk}
\end{split}
\end{equation}

\begin{equation} \label{eq:e6_deriv}
\begin{split}
	\E[\ln q(\boldbeta, \boldtau)] &= \ksum \E \Big[ \ln \mathcal{N} \left( \beta_k \given m_k, (\tau_k V_k)^{-1}\right) + \ln \mathrm{Ga} \left( \tau_k \given a_k, b_k \right)\Big] \\
	&= \ksum -\frac{D}{2} \ln(2\pi) + \frac{1}{2} \ln |V_k| + \frac{D}{2} \E [\ln \tau_k] - \E \Big[ \frac{\tau_k}{2}\transpose{(\beta_k - m_k)} V_k (\beta_k - m_k) \Big] \\
	& \quad \quad + \ln \frac{b_k^{a_k}}{\Gamma(a_k)} + (a_k - 1) \E[ \ln \tau_k] - b_k \E[\tau_k] \\
	&= -\frac{D}{2} \ln (2\pi) + \frac{1}{2} \ln |V_k| + C(a_k, b_k)
\end{split}
\end{equation}
where $C(a_k, b_k) = \left(\frac{D}{2} + a_k - 1)(\psi(a_k) - \psi(b_k)\right) + \ln \frac{b_k^{a_k}}{\Gamma(a_k)} - \frac{D}{2} - a_k$

\begin{equation} \label{eq:e7_deriv}
\begin{split}
	\E[\ln q(\boldgamma)] &= \sum_{k} \ln|\mathrm{Q}_k| - \frac{D}{2}(\ln (2\pi) + 1)
\end{split}
\end{equation}



\vskip 0.2in
\bibliography{sample}

\end{document}