\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{hyperref}	

\usepackage{tikz}
\usetikzlibrary{bayesnet}

\usepackage{algorithm}
\usepackage[options ]{algorithm2e}
\usepackage[noend]{algpseudocode}

\setlength{\parindent}{0pt}




% Definitions of handy macros can go here

%\usepackage[style=authoryear]{biblatex}
%\addbibresource{sample.bib}
%\renewcommand*{\nameyeardelim}{\addcomma\space}

\usepackage[style=authoryear]{biblatex}
\setlength\bibitemsep{2\itemsep}
\addbibresource{sample.bib}
\renewcommand*{\nameyeardelim}{\addcomma\addspace}


\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\tr}{\intercal}
\newcommand{\eye}{\mathrm{I}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\transpose}[1]{#1^{\intercal}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\nprod}{\prod\limits_{n}}
\newcommand{\kprod}{\prod\limits_{k}}
\newcommand{\nsum}{\sum\limits_{n}}
\newcommand{\ksum}{\sum\limits_{k}}
\newcommand{\boldbeta}{\boldsymbol\beta}
\newcommand{\boldgamma}{\boldsymbol\gamma}
\newcommand{\boldomega}{\boldsymbol\omega}
\newcommand{\boldtau}{\boldsymbol\tau}
\newcommand{\sumexp}{\sum_{j=1}^{K} \exp \{ \transpose{x_n} \gamma_j \}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\diagdots}{_{^{\big\cdot}\cdot _{\big\cdot}}}
\newcommand{\iid}{\overset{i.i.d}{\sim}}
% \newcommand{\var}{\mathrm{Var}}

\newcommand{\betad}{\tilde{\beta}_d}
\newcommand{\betaj}{\tilde{\beta}_j}
\newcommand{\umat}{\mathrm{U}}
\newcommand{\qmat}{\mathrm{Q}}

\newcommand{\priorbeta}{\mathcal{N} \left( \betad \given 0, \xi_0^{-1} \cdot \mathrm{I}_K \right)}
\newcommand{\qbeta}{\mathcal{N} \left( \betad \given m_d, \qmat_d^{-1} \right)}


\usepackage{setspace}
\let\Algorithm\algorithm
\renewcommand\algorithm[1][]{\Algorithm[#1]\setstretch{1.05}}


\newcommand{\pr}[1]{p \left( #1 \right)}
\newcommand{\trace}[1]{\mathrm{tr} \left( #1 \right)}
\newcommand{\var}[1]{\mathrm{Var}\left(#1\right)}


% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

% \ShortHeadings{A Variational Approach for Bayesian Density Regression}{}
\firstpageno{1}

\begin{document}

\title{A Variational Approach for Bayesian Density Regression}

\author{\name Eric Chuu \email ericchuu@stat.tamu.edu \\
       \addr Department of Statistics\\
       Texas A\&M University \\
       College Station, TX 77840, USA}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
In the Bayesian density regression problem, a mixture of experts model is often used because of the high flexibility in estimating conditional densities. In this paper, we discuss the case when covariate dependent weights are used in the approximating mixture density. Under this framework, however, traditional Bayesian methods result in computational difficulties when the dimension of the covariates is large. In order to remedy this problem and to provide a method for faster inference, we propose using a variational approximation to estimate the conditional density. We also discuss upper bounds for approximating quantities that lack a closed form so that a coordinate ascent algorithm is viable.
\end{abstract}

\begin{keywords}
  Bayesian Density Regression, Variational Bayes, Mixture Models
\end{keywords}

\section{Introduction}

In the Bayesian density regression problem, we observe data $\left(y_n, x_n \right)_{n=1}^N$, and the goal is the estimate the conditional density of $y \given x$. A common approach for doing this is to model the density using a mixture of gaussians, such as the following,
\begin{equation} \label{eq:general_gm}
	f(y \given x) = \sum_{k} \pi_k \mathcal{N} \left(y \given \mu_k(x), \tau_k^{-1} \right)
\end{equation}

While the representation of the density using predictor-independent weights yields less expensive computation, this approach often lacks flexibility to make it useful in practice and results in a reliance on have too many mixture components. As a result, there have been many proposed models that consider predictor-dependent weights. Some examples include using a kernel stick-breaking process \parencite{dunsonpark:08} or logit stick-breaking prior \parencite{durante:17} to generate the covariate-dependent weights. In the former method, the increased flexibility comes at heavy computational cost, and in the latter method, the process from which the weights are generated does not allow for intuitive inference. In our proposed model, the covariates enter the weights through a logistic link function so that we can naturally perform inference on the coefficients. More specifically, we can model
\begin{equation} \label{eq:covdep_gm}
	 f(y \given x) = \sum_{k}^{K} \pi_k(x) \mathcal{N} \left( y \given \mu_k(x), \tau_k^{-1} \right) 
\end{equation} 
where and $\pi_{k} \propto \exp(\transpose{x} \gamma_k)$. In order to perform fast inference on the model parameters, we adopt a variational approach to obtain an approximating distribution to the true posterior. Using this covariate-dependent setup, however, introduces a problem in the traditional coordinate ascent algorithm such that it prevents closed form updates of the variational distributions. In section \ref{sec:priors}, we give an overview of the priors used in the problem. In section \ref{sec:variational}, we consider the family of variational distributions that we use to approximate the true posterior. We then propose a way to obtain closed form updates by considering an upper bound on the problematic quantity, and in section \ref{alg1}, we formulate the complete algorithm. Finally, we discuss potential shortcomings of the proposed algorithm and other bounds that could be used to obtain more accurate approximating distributions.

\section{Notation and Prior Specification} \label{sec:priors}
For the set of observed data, we denote $\mathbf{y} = \{y_1, \ldots, y_N \}, \mathbf{X} = \{ x_1, \ldots, x_N \}$, where each $x_n \in \R^{D}$. Let $\boldbeta = \{ \beta_1, \ldots, \beta_K\}$ and $\boldgamma = \{ \gamma_1, \ldots, \gamma_k\}$ denote the $D$-dimensional coefficient vectors  in the mixture weights and in gaussian mixture components, respectively. Finally, let $\boldtau = \{ \tau_1, \ldots, \tau_k \}$ denote the precision parameters. The mixture density can then be written explicitly as

\begin{equation} \label{eq:mix_density}
	 \pr{y_n \given x_n, \boldbeta, \boldtau} = \ksum \pi_k(x_n) \mathcal{N} \left( y_n \given \transpose{x_n}\beta_k, \tau_k^{-1} \right), \quad \pi_k(x_n) = \frac{\exp\{\transpose{x_n} \gamma_k\}}{\sum\limits_{j=1}^{K} \exp\{ \transpose{x_n} \gamma_j\}}
\end{equation}

We can simplify the form of the density by introducing the set of latent variables $\mathbf{Z} = \{ z_1, \ldots, z_N \}$, where $z_n \in \R^K$ and $z_{nk} = 1$ if and only if $y_n$ belongs to the $k$-th cluster so that $\sum_{k} z_{nk} = 1$. Conditioning on this additional variable $\mathbf{Z}$, we have the following density, 

%% should probably include a figure displaying the graphical model, similar ot the one shown on p.475 (Bishop)

\begin{equation} \label{eq:simp_lik}
	p \left( \mathbf{y} \given \mathbf{X}, \boldsymbol\beta, \boldsymbol{\tau}, \mathbf{Z} \right) = 
	\prod_{n} \prod_{k} \mathcal{N} \left( y_n \given \transpose{x_n} \beta_k, \tau_{k}^{-1} \right)^{z_{nk}}
\end{equation}
\begin{equation} \label{eq:z_prior}
	p \left( \mathbf{Z} \given \mathbf{X}, \boldsymbol\gamma \right) = \nprod \kprod \pi_{k} (x_n)^{z_{nk}} = 
	\nprod \kprod \left( \frac{\exp\{\transpose{x_n} \gamma_k\}}{\sumexp}\right)^{z_{nk}}
\end{equation}

Next, we introduce the priors over the parameters $\boldbeta, \boldtau, \boldgamma$, where we simplify the calculations by considering conjugate priors. For ease of computation, we consider an independent standard normal prior on $\gamma_k$'s, given by

\begin{equation} \label{eq:gamma_prior}
\begin{split}
	 p(\boldgamma) = \kprod p(\gamma_k) = \kprod \mathcal{N} \left( \gamma_k \given 0, \eye_D \right)
\end{split}
\end{equation}

For $(\boldbeta, \boldtau)$, we consider an independent normal-gamma prior, given by

\begin{equation} \label{eq:betatau_prior}
\begin{split}
	 p\left( \boldbeta, \boldtau \right) = \pr{\boldbeta \given \boldtau} = \prod_{k} \kprod \mathcal{N} \left( \beta_k \given m_0, (\tau_k \Lambda_0)^{-1} \right) \mathrm{Ga} \left( \tau_k \given a_0, b_0 \right)
\end{split}
\end{equation}

Note that in the case where the mixture weights are covariate-independent, a Dirichlet prior is typically used for the mixing weights, $\pi_1, \ldots, \pi_K$. In this case, however, the mixing weights are fully specified by $\mathbf{X}$ and $\boldgamma$, so we need only place a prior on $\boldgamma$. 


\section{Variational Distribution} \label{sec:variational}
At this point, the variational parameters of interest are $\boldsymbol \theta = (\mathbf{Z}, \boldbeta, \boldtau, \boldgamma)$. The log of the joint distribution of these random variables (written up to constants) is given by
\begin{equation} \label{eq:joint}
\begin{split}
	\ln  \pr{\mathbf{y}, \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}} &= 
	\ln \bigg\{\pr{\mathbf{y} \given \mathbf{X}, \boldsymbol\beta, \boldsymbol{\tau}, \mathbf{Z}, \boldsymbol\gamma}\pr{\mathbf{Z}\given \mathbf{X}, \boldgamma}\pr{\boldgamma}\pr{\boldbeta, \boldtau} \bigg\}\\
	&= \nsum \ksum z_{nk}\bigg\{ -\frac{1}{2}\ln(2\pi) + \frac{1}{2} \ln \tau_k - \frac{\tau_k}{2} \left( y_n - \transpose{x_n}\beta_k\right)^2 \bigg\} \\
	& + \nsum \ksum z_{nk} \bigg\{ \transpose{x_n} \gamma_k - \ln \sumexp  \bigg\} + \ksum \bigg\{ -\frac{D}{2} \ln (2\pi) - \frac{1}{2} \transpose{\gamma_k}\gamma_k \bigg\} \\
	& + \ksum  \bigg\{ -\frac{D}{2} \ln (2\pi) + \frac{D}{2} \ln \tau_k + \ln | \Lambda_0| - \frac{\tau_k}{2}\transpose{(\beta_k - m_0)} \Lambda_0 (\beta_k - m_0)\bigg\} \\
	& + \ksum \bigg\{ (a_0 - 1) \ln \tau_k - b_0 \tau_k \bigg\}
\end{split}
\end{equation}

We consider the following variational distribution used to approximate the posterior distribution of the parameters outlined previously.
\begin{equation} \label{eq:variational}
	q \left( \mathbf{Z}, \boldbeta, \boldtau, \boldgamma \right) = q(\mathbf{Z}) q(\boldbeta, \boldtau, \boldgamma)
\end{equation}

\subsection{Coordinate Ascent Updates} \label{sec:cavi}

As is standard in variational algorithms, we now seek the sequential updates of the factors in (\ref{eq:variational}). For a particular variational parameter, the optimal distribution is found by taking the expectation of the joint distribution of all the random variables with respect to all of the \textit{other} variational parameters, excluding the one of interest \parencite{bishop:06}. Proceeding this way, we arrive at the following update equation for $q(\mathbf{Z})$,
\begin{equation} \label{eq:ln_z_update}
	\ln q^{*}(\mathbf{Z}) = \E_{-q(\mathbf{Z})} \Big[ \ln \big\{ \pr{\mathbf{y}, \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}, \boldgamma} \big\} \Big]
\end{equation}
We adopt the convention that the expectation with respect to a negative subscript indicates an expectation taken with respect to the other variational parameters. Ignoring terms that are not functionally dependent on $\mathbf{Z}$, we can exponentiate both sides of (\ref{eq:ln_z_update}) to obtain the optimal solution for $q(\mathbf{Z})$,
\begin{equation} \label{optimal_z}
	 q^{*}(\mathbf{Z}) = \prod_{n} \prod_{k} r_{nk}^{z_{nk}}, \quad r_{nk} = \frac{\rho_{nk}}{\sum_{j} \rho_{nj}}
\end{equation}
For the details in this calculation, refer to Appendix \ref{app:q_z}. The form of this discrete distribution gives us $\E[z_{nk}] = r_{nk}$. If we consider the quantity $\ln \rho_{nk}$, defined in (\ref{eq:problem}) and discussed in more detail in Appendix \ref{app:q_z}, we note that the exact computation of $\ln \rho_{nk}$ involves four expectations taken with respect to the variational distribution $q(\boldbeta, \boldtau, \boldgamma)$. 

\begin{equation} \label{eq:problem}
\begin{split}
 \ln \rho_{nk} = &-\frac{1}{2}\ln(2\pi) + \frac{1}{2} \E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} \E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2] \\ 
	& + x_n^{\tr}\E_{q(\boldsymbol\gamma)}[\gamma_k] - \E_{q(\boldsymbol\gamma)}\Bigg[\ln  \sum_{j} \exp \{ x_n^{\tr} \gamma_j \}\Bigg]
\end{split}
\end{equation}

We briefly consider each of these expectations. Since we used conjugate families, we know that $\E_{q(\boldsymbol\tau)}[ \ln \tau_k ], \E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2]$, and $\E_{q(\boldsymbol\gamma)}[\gamma_k]$ will have closed form expressions. The remaining expectation, 
$\E_{q(\boldsymbol\gamma)}\big[\ln \sum_{j} \exp \{ x_n^{\tr} \gamma_j \} \big]$ presents a problem in that there lacks a closed form expression. Therefore, in order to complete this update, we use the following upper bound to approximate this quantity,


\begin{equation} \label{eq:bouchard_approx}
	\E_{q(\boldsymbol\gamma)}\big[\ln \sum_{j} \exp \{ x_n^{\tr} \gamma_j \} \big] \approx \alpha_n + \varphi_n
\end{equation}

where $\varphi_n =  \sum\limits_{j}^K \frac{1}{2}\left(x_n^{\tr}\mu_j - \alpha_n + \xi_{nj}\right) + \log( 1 + e^{\xi_{nj}})$. The details for the approximation and how to find $\alpha_n$ and $\xi_{nj}$ can be found in Appendix \ref{app:gamma}. Using this in place of the problematic expectation in (\ref{eq:problem}), we are able to obtain $r_{nk}$ in closed form. \\

The remaining variational distribution can be found by considering the expectation of the joint density in (\ref{eq:joint}) taken with respect to $q(\mathbf{Z})$, as derived above. The resulting variational distribution can be written up to constants as shown below,
\begin{equation} \label{eq:ln_beta_tau_gamma}
\begin{split}
	\ln q^{*}(\boldbeta, \boldtau, \boldgamma) &=  \ksum \nsum  \E_{q(\mathbf{Z})}[z_{nk}] \ln \mathcal{N} \left( y_n \given \transpose{x_n} \beta_k, \tau_{k}^{-1} \right) + \ksum \ln \pr{\beta_k, \tau_k} \\
	& + \ksum \nsum \E_{q(\mathbf{Z})}[z_{nk}] \left( \transpose{x_n} \gamma_k - \ln  \sumexp \right) + \ksum \ln \mathcal{N}(\gamma_k \given 0, \eye_D)
\end{split}
\end{equation}

Having just deriving the form for $q(\mathbf{Z})$, we can write out closed forms for the two expectation terms above. Noting in the summations above that the optimal distribution for $q(\boldbeta, \boldtau, \boldgamma)$ has a sum involving only the $\gamma_k$'s in the second line and a sum involving only the $(\beta_k, \tau_k)$'s in the first line of (\ref{eq:ln_beta_tau_gamma}), we can deduce $q(\boldbeta, \boldtau, \boldgamma) = \kprod q(\beta_k, \tau_k) q(\gamma_k)$ and find the optimal distributions separately. With this factorization in mind, we obtain the following updates for the remaining variational parameters,
\begin{equation} \label{optimal_beta_given_tau}
	q^{*}(\beta_k \given \tau_k) = \mathcal{N}\left(\beta_k \given m_k, (\tau_k \mathrm{Q}_k)^{-1} \right)
\end{equation}

\begin{equation} \label{optimal_tau}
	q^{*}(\tau_k) =  \mathrm{Ga}\left( \tau_k \given a_k, b_k \right)
\end{equation}

\begin{equation} \label{optimal_gamma}
	q^{*}(\gamma_k) = \mathcal{N} \left( \gamma_k \given \mu_k, \mathrm{V}_k^{-1} \right)
\end{equation}

for $k = 1, \ldots, K$. The derivation and parameter definitions for (\ref{optimal_beta_given_tau}) and (\ref{optimal_tau}) can be found in Appendix \ref{app:beta_tau}, and the details for (\ref{optimal_gamma}) can be found in Appendix \ref{app:gamma}. Now that we have closed form updates for the variational distributions $q(\mathbf{Z})$ and $q(\boldbeta, \boldtau, \boldgamma)$, we can cycle through a two-step update procedure. In the first step (variational E-step), we compute the expectation of each of the $z_{nk}$'s using the variational distributions (\ref{optimal_beta_given_tau}), (\ref{optimal_tau}), and (\ref{optimal_gamma}). Then in the second step (variational M-step), we derive new optimal distributions using results from the E-step. We alternate between these two steps until the variational lower bound converges, as discussed in the section \ref{sub:elbo}. 


\subsection{Evidence Lower Bound (ELBO)} \label{sub:elbo}
In order to evaluate the convergence of the coordinate ascent algorithm, we can calculate the evidence lower bound using the updated variational parameters at the end of each iteration. Since the ELBO is monotonic increasing, we continue the coordinate ascent until the change in the ELBO between iterations falls below a predetermined tolerance. Note that the expectations taken below are with respect to the optimal variational distributions defined in the previous section.

\begin{equation} \label{eq:ELBO}
\setlength{\jot}{11pt}
\begin{split}
	\mathcal{L}(q) &= \sum_{z} \int \int \int q(\boldbeta, \boldtau, \boldgamma, \mathbf{Z})
	\ln \Bigg\{ \frac{p(\mathbf{y}, \mathbf{X}, \boldbeta, \boldtau, \boldgamma, \mathbf{Z})}{q(\boldbeta, \boldtau, \boldgamma, \mathbf{Z})}\Bigg\} d\boldbeta d\boldtau d\boldgamma \\
	&= \E[\ln\pr{\mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}}] + 
	\E[\ln \pr{\mathbf{Z} \given \mathbf{X}, \boldgamma}] + \E[\ln \pr{\boldgamma}] + \E[\ln \pr{\boldbeta, \boldtau}] \\
	& \quad - \E[\ln q(\mathbf{Z})] - \E[\ln q(\boldbeta, \boldtau)] - \E[\ln q(\boldgamma)]
\end{split}
\end{equation}

Details for each of the expectations can be found in Appendix \ref{app:elbo}. 
\begin{align*}
	& \E[\ln\pr{\mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}}] = -\frac{1}{2}\nsum \ksum r_{nk} \Big\{ \ln (2\pi) - \psi(a_k) + \psi(b_k)  + \frac{a_k}{b_k}(y_n - \transpose{x_n}m_k)^2 + \transpose{x_n}\mathrm{Q}_k^{-1}x_n \Big\} \\
	 & \E[\ln \pr{\mathbf{Z} \given \mathbf{X}, \boldgamma}] = \nsum \ksum r_{nk} \left( \transpose{x_n}\mu_k - \alpha_n - \varphi_n \right) \\
	 & \E[\ln \pr{\boldgamma}] = - \frac{K \cdot D}{2} \ln (2\pi) - \frac{1}{2} \ksum \transpose{\mu_k}\mu_k \\
	 & \E[\ln \pr{\boldbeta, \boldtau}] = - K \left(\frac{D}{2}\ln(2\pi)  - \ln |\Lambda_0| - a_0 \ln b_0 + \ln \Gamma(\alpha_0) \right) + \left( a_0 + \frac{D}{2} - 1 \right) \ksum \psi(a_k) - \psi(b_k) \\
	   & \qquad \qquad \qquad \qquad - \frac{1}{2}\ksum \Bigg\{ \frac{a_k}{b_k}\Big[\transpose{(m_k - m_0)} \Lambda_0 (m_k - m_0) + b_0 \Big] + \mathrm{tr} \left( \Lambda_0 \mathrm{Q}_{k}^{-1}\right) \Bigg\} \\
	& \E[\ln q(\mathbf{Z})] =  \nsum \ksum r_{nk} \ln r_{nk} \\
	& \E[\ln q(\boldbeta, \boldtau)] =  \ksum \left( \frac{D}{2} + a_k - 1 \right) \big[\psi(a_k) - \psi(b_k) \big] + a_k (\ln b_k - 1) - \ln \Gamma(a_k) + \ln |\mathrm{Q}_k | \\
	& \qquad \qquad \qquad \qquad -\frac{KD}{2} \left( \ln(2\pi) + 1 \right) \\
	& \E[\ln q(\boldgamma)] = - \frac{KD}{2}(\ln (2\pi) + 1) + \sum_{k} \ln|\mathrm{V}_k|
\end{align*}

\newpage

\section{Algorithm} \label{alg1}

Using the updates discussed in the previous section, we can formalize the variational algorithm below. 
% Note that in the update of the $r_{nk}$'s, we use the approximation given in (\ref{eq:bouchard_approx}) to compute the update in closed form. 
\begin{algorithm}[H]
\SetAlgoLined
\KwResult{An approximating distribution to the true posterior of \boldsymbol\theta}
\textbf{Input:} Data $y_{1:N}, x_{1:N}$, number of components $K$, prior mean and precision for $\boldbeta_{1:K}$, prior shape, rate parameters for precision parameters $\tau_{1:K}$\;

\textbf{Output:} A variational density $q \left( \mathbf{Z}, \boldbeta, \boldtau, \boldgamma \right) = q(\mathbf{Z}) q(\boldbeta, \boldtau, \boldgamma) = q(\mathbf{Z}) \kprod q(\beta_k, \tau_k) q(\gamma_k)$;\

\textbf{Initialize:} Variational parameters $\mathbf{m}_{1:K}, \mathrm{V}_{1:K}, \boldsymbol\mu_{1:K}, \mathrm{Q}_{1:K}, a_{1:K}, b_{1:K}, \xi_{1:N, 1:K}, \alpha_{1:N}$

 \While{the ELBO has not converged}{
  
  \For{$n \in \{1, \ldots, N\}$}{
       \For{$k \in \{1, \ldots, K\}$}{
       Set $r_{nk} \propto \exp \Big\{-\frac{1}{2} \ln (2\pi) + \frac{1}{2} \E[\ln \tau_k]
       - \frac{1}{2} \E [\tau_k(y_n - \transpose{x_n}\beta_k)^2]\big + \transpose{x_n} \E[\gamma_k]$ \\
       $\qquad - \E\Big[\ln \left( \sumexp \right)\Big]\Big\}$\;
       }
   }
  \texttt{}

  %% update variational parameters for determining variational distribution of gamma
  \For{$n \in \{1, \ldots, N\}$}{ %% order of the following updates matters
      \For{$k \in \{1, \ldots, K\}$}{
          % update xi_1k, ... xi_Nk
	      Set $\xi_{nk} \leftarrow \sqrt{(\transpose{x_n}\mu_k - \alpha_n)^2 + \transpose{x_n}\mathrm{Q}_k^{-1} x_n}$\;          
       }
       
       Set $\alpha_{n} \leftarrow \dfrac{\frac{1}{2} \left( \frac{K}{2} - 1\right) + \ksum \lambda(\xi_{nk}) \transpose{\mu_k} x_n }{\sum_{k} \lambda(\xi_{nk})}$\;
   }

   
   \texttt{}
   
   \For{$k \in \{1, \ldots, K\}$}{ %% order of the following updates matters
   
   \texttt{}
   
   Set $\mathrm{Q}_{k} \leftarrow \eye_D + 2 \sum_{n} r_{nk} \lambda(\xi_{nk}) x_n \transpose{x_n}$\; % update Q_k
   
   Set $\eta_k \leftarrow \sum_{n} r_{nk} \big[ \frac{1}{2} + 2\lambda(\xi_{nk}) \alpha_n \big]x_n$\; % update eta_k
         
   Set $\mu_k \leftarrow \mathrm{Q}_k^{-1} \eta_k$\; % update mu_k = Q_k^{-1} eta_k
   
   Set $\mathrm{V}_k \leftarrow \nsum r_{nk} x_n \transpose{x_n} + \Lambda_0$ \; % update V_k (update parameters of beta don't directly invvolve tau_k)
   
   Set $\zeta_k \leftarrow \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0$\; % update mean_of_beta_k (update parameters of beta don't directly invvolve tau_k)
   
   Set $m_k \leftarrow \mathrm{V}_k^{-1} \zeta_k$\; % update m_k (update parameters of beta don't directly invvolve tau_k)
   
   Set $a_k \leftarrow a_0 + N_k$\; % update a_k
   
   Set $b_k \leftarrow b_0 + \frac{1}{2}[\nsum r_{nk} y_n^2 + \transpose{m_0}\Lambda_0 m_0 - \transpose{\zeta_k} \mathrm{V}_k^{-1} \zeta_k]$\; % update b_k --> this one last since it involves V_k, and mean of beta_k
   }
   Compute ELBO using updated parameters
 } % end of while
 \Return $q \left( \mathbf{Z}, \boldbeta, \boldtau, \boldgamma \right)$
 \caption{CAVI for Conditional Density Estimation}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
In this project, we focused on one primary approximation to the difficult-to-compute expectation of the log sum of exponentials, $\E_{q(\boldsymbol\gamma)}\big[\ln \sum_{j} \exp \{ x_n^{\tr} \gamma_j \} \big]$. 
Alternatively, we also considered a simpler bound in section \ref{sec:alt}, which only requires us to compute moment generating functions. As has been noted in previous work and simulations, the former bound provides better approximations when the variance of $q(\boldgamma)$ is extremely large because the approximation is asymptotically optimal \parencite{bouchard:07}, but in most other cases, the performance deteriorated \parencite{Depraetere:17}. Although both of these methods require additional variational parameters, this increase in model complexity is offset by the faster inference that variational Bayesian methods provide. As a result, even though each iteration involves $2K$ ($D \times D$)-matrix inversions to update the variational distributions for $\beta_k$ and $\gamma_k$, we expect the number of iterations in the coordinate ascent algorithm to be far fewer than if we use a traditional Gibbs sampling scheme. \\

While we only considered two candidate approximations in this project, there are numerous ways to approach the problem. Although in a slightly different context, Depraeteret and Vandebroek (2017) provide several approximations to the same quantity of interest, all of which admit closed form updates so that variational Bayesian methods are tractable. 


% Acknowledgements should go at the end, before appendices and references

\newpage

\section{Variable Selection for Gaussian Component} \label{vs}  %% variable selection


\subsection{Notation and Prior Specification}
Recall the previous setup, where we denote $\mathbf{y} = \{y_1, \ldots, y_N \}, \mathbf{X} = \{ x_1, \ldots, x_N \}$, where each $x_n \in \R^{D}$. Let $\boldbeta = \{ \beta_1, \ldots, \beta_K\}$ and $\boldgamma = \{ \gamma_1, \ldots, \gamma_k\}$ denote the $D$-dimensional coefficient vectors  in the mixture weights and in gaussian mixture components, respectively. Finally, let $\boldtau = \{ \tau_1, \ldots, \tau_k \}$ denote the precision parameters for each of the gaussian components in the mixture density. The mixture density can then be written explicitly as

\begin{equation} 
	 \pr{y_n \given x_n, \boldbeta, \boldtau} = \ksum \pi_k(x_n) \mathcal{N} \left( y_n \given \transpose{x_n}\beta_k, \tau_k^{-1} \right), \quad \pi_k(x_n) = \frac{\exp\{\transpose{x_n} \gamma_k\}}{\sum\limits_{j=1}^{K} \exp\{ \transpose{x_n} \gamma_j\}}
\end{equation}

% include picture of graphical model

Conditioning on the same set of latent variables, $\mathbf{Z} = \{ z_1, \ldots, z_N \}$, used indicate the membership for each of the response variables, we obtain the mixture density in product form. 

\begin{equation} 
	p \left( \mathbf{y} \given \mathbf{X}, \boldsymbol\beta, \boldsymbol{\tau}, \mathbf{Z} \right) = 
	\prod_{n} \prod_{k} \mathcal{N} \left( y_n \given \transpose{x_n} \beta_k, \tau_{k}^{-1} \right)^{z_{nk}}
\end{equation}
\begin{equation} 
	p \left( \mathbf{Z} \given \mathbf{X}, \boldsymbol\gamma \right) = \nprod \kprod \pi_{k} (x_n)^{z_{nk}} = 
	\nprod \kprod \left( \frac{\exp\{\transpose{x_n} \gamma_k\}}{\sumexp}\right)^{z_{nk}}
\end{equation}

For $\boldgamma$, we consider an independent standard normal prior on the $\gamma_k$'s, given by
\begin{equation}
\begin{split}
	 p(\boldgamma) = \kprod p(\gamma_k) = \kprod \mathcal{N} \left( \gamma_k \given 0, \eye_D \right)
\end{split}
\end{equation}

Instead of the normal-gamma prior that we used in the standard setup, we place a gamma prior on $\tau_k$'s: 
\begin{equation}
	p(\boldtau) = \kprod p(\tau_k) = \kprod \mathrm{Ga} \left( \tau_k \given a_0, b_0 \right)
\end{equation}



Finally, to incorporate variable selection we consider the \textit{rows} of $\boldbeta$, as follows:

\[
\boldbeta = 
\begin{bmatrix}
    \vert &    \vert   &   &\vert\\
    \beta_1  & \beta_2 &   \ldots & \beta_K\\
    \vert &     \vert  &    &\vert
\end{bmatrix}
= 
\begin{bmatrix}
    \text{---} \hspace{-0.2cm} & \tilde{\beta}_1 & \hspace{-0.2cm} \text{---} \\
    \text{   } \hspace{-0.2cm} & \vdots          & \hspace{-0.2cm} \text{   } \\
    \text{---} \hspace{-0.2cm} & \tilde{\beta}_D & \hspace{-0.2cm} \text{---}
\end{bmatrix}
\]

Using the above formulation, $\betad = \transpose{\left( \beta_{1d}, \beta_{2d}, \ldots, \beta_{Kd}\right)} \in \R^K$ represents the $d$-th row of the coefficient matrix. In other words, $\betad$ represents the coefficient vector for the $d$-th predictor across all $K$ clusters. We place independent spike and slab priors on each \textit{row} vectors of $\boldbeta$, 

\begin{equation} 
	\betad \sim \pi \priorbeta + (1 - \pi) \delta_0 (\betad)
\end{equation}

for $\betad$ for $d = 1, \ldots, D$. \\

Denote $\boldsymbol \omega = \{ \omega_1, \ldots, \omega_D\}$, where $\omega_d \iid \mathrm{Ber}(\omega_d \given \pi)$, we have the following reparametrization of the spike and slab prior on $\boldbeta$:
\begin{equation} \label{eq:beta_joint_prior_vs}
	\pr{\boldbeta, \boldsymbol\omega} = \prod_{d} \pr{\betad, \omega_d} = \prod_{d} \priorbeta^{\omega_d} \pi^{\omega_d} (1-\pi)^{1 - \omega_d}
\end{equation}

\subsection{Joint Likelihood}
Using the priors outlined in the previous section, we have the following expression for the joint likelihood. 
\begin{equation} \label{eq:joint_vs}
\begin{split}
	\ln  \pr{\mathbf{y}, \mathbf{X}, \boldbeta, \boldsymbol\omega, \boldtau, \mathbf{Z}} &= 
	\ln \bigg\{\pr{\mathbf{y} \given \mathbf{X}, \boldsymbol\beta, \boldsymbol{\tau}, \mathbf{Z}, \boldsymbol\gamma}\pr{\mathbf{Z}\given \mathbf{X}, \boldgamma}\pr{\boldgamma}\pr{\boldbeta, \boldsymbol\omega}\pr{\tau} \bigg\}\\
	&= \nsum \ksum z_{nk}\bigg\{ -\frac{1}{2}\ln(2\pi) + \frac{1}{2} \ln \tau_k - \frac{\tau_k}{2} \left( y_n - \transpose{x_n}\beta_k\right)^2 \bigg\} \\
	& + \nsum \ksum z_{nk} \bigg\{ \transpose{x_n} \gamma_k - \ln \sumexp  \bigg\} + \ksum \bigg\{ -\frac{D}{2} \ln (2\pi) - \frac{1}{2} \transpose{\gamma_k}\gamma_k \bigg\} \\
	& + \sum_{d} \omega_d \bigg\{ -\frac{K}{2} \ln(2\pi) + \frac{K}{2} \ln \xi_0 - \frac{\xi_0}{2} \transpose{\betad} \betad\bigg\} + \omega_d \ln \pi + ( 1- \omega_d) \ln(1 - \pi) \\
	& + \ksum \bigg\{ (a_0 - 1) \ln \tau_k - b_0 \tau_k \bigg\}
\end{split}
\end{equation}


\subsection{Approximating Distribution}

%% both huang and CS have a blurb about q being the minimizer of the KL divergence
%% include something of a similar flavor here

The variational parameters of interest are $\boldsymbol \theta = \left(\mathbf{Z}, \boldtau, \boldbeta, \boldgamma, \boldsymbol \omega \right)$. We consider an approximating distribution of the following form
\begin{equation} \label{q_vs}
	q(\boldsymbol \theta) = q \left(\mathbf{Z}, \boldtau, \boldbeta, \boldgamma, \boldsymbol \omega \right) = q(\mathbf{Z}) \cdot q(\boldtau, \boldgamma) \cdot \prod_{d} q(\betad, \omega_d)
\end{equation}
In fact, it follows easily (without assuming anything about the functional forms) that the right hand side of  (\ref{q_vs}) has the following induced factorization, 
\begin{equation} \label{q_vs_fact}
	q \left(\mathbf{Z}, \boldtau, \boldbeta, \boldgamma, \boldsymbol \omega \right) =  \prod_{n} \prod_{k} q(z_{nk}) \cdot \prod_{k} q(\tau_k) \cdot \prod_{k} q(\gamma_k) \cdot \prod_{d} q(\betad, \omega_d)
\end{equation}
From our choice of conjugate families in the prior distributions, each of variational parameters, with the exception of $q \left( \boldgamma \right)$, can be updated in closed form. Proceeding in standard fashion by taking the expectation with respect to the variational distribution, we can derive the functional form for each of the approximating distributions and their corresponding coordinate ascent updates. \\

% include joint likelihood here since it is used explicitly to compute all of the following updates


\subsection{Coorindate Ascent Algorithm: Component-wise VB}
While the updates for $\{\mathbf{Z}, \boldtau, \boldgamma \}$ are performed in a similar manner as the standard algorithm. the variable selection part of the algorithm iterates over $D$ dimensions, updating the features sequentially. \\


\textbf{Update $q \left( z_{nk} \right)$}. As shown in the previous case, we know the optimal distribution for $\mathbf{Z}$ is of the form
$$q(z_{nk}) = r_{nk}^{z_{nk}}, \quad r_{nk} = \frac{\rho_{nk}}{\sum_{j} \rho_{nj}}$$ 
Note that $\rho_{nk}$ involves evaluating a number of expectations taken with respect to various variational distribution, each of which is updated during sequentially during each iteration of the coordinate ascent algorithm. In particular, 

\begin{equation} \label{eq:ln_rho_vs}
\begin{split}
 \ln \rho_{nk} = &-\frac{1}{2}\ln(2\pi) + \frac{1}{2} \E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} \E_{q(\boldsymbol\tau)}[\tau_k] \cdot \E_{q(\boldbeta, \boldsymbol\omega))}[(y_n - x_n^{\tr}\beta_k)^2] \\ 
	& + x_n^{\tr}\E_{q(\boldsymbol\gamma)}[\gamma_k] - \E_{q(\boldsymbol\gamma)}\Bigg[\ln \sum_{j}^K \exp \{ x_n^{\tr} \gamma_j \}\Bigg]
\end{split}
\end{equation}

so that $\E[z_{nk}] = r_{nk}$. Explicit expressions for each of the expectations, which can either be computed in closed form or approximated, can be found in the supplemental section. \\


\textbf{Update $q \left(  \tau_k \right)$}. For $k \in \{1, \ldots, K \}$, $q(\tau_k) = \mathrm{Ga}\left( \tau_k \given a_k, b_k \right)$. The shape and rate parameters can be updated with
\begin{align}
	a_k &= a_0 + \frac{N_k}{2} - 1 \label{eq:ak_vs} \\
	b_k &= b_0 + \frac{1}{2} \nsum (y_n -\transpose{x_n} \tilde{m}_k)^2 + \transpose{x_n}\var{\beta_k} x_n \label{eq:bk_vs}
\end{align}
where $\tilde{m}_k = $ and $\var{\beta_k} = $. \\

\textbf{Update $q \left(  \gamma_k \right)$}. The update for $\boldgamma$ remains the same as before:
\begin{equation} \label{optimal_gmama_vs}
	q(\gamma_k) = \mathcal{N} \left( \gamma_k \given \mu_k, \mathrm{V}_k^{-1} \right)
\end{equation}



\textbf{Update $q ( \betad, \omega_d )$}. We do this sequentially by first deriving the update for $q\left(\betad \given \omega_d = 1 \right)$ and then for $q\left( \omega_d \right)$. It is helpful to rewrite the first term in the log joint likelihood (mixture density) in terms of $\betad$. Conditioning on $\omega_d = 1$, we have the variational distribution with equality written up to constants (in $\betad)$, 
\begin{align}
	 \ln q\left(\betad \given \omega_d = 1 \right) &= - \frac{1}{2} \transpose{\betad} \umat_d \betad + \transpose{\betad} \eta_d - \frac{\xi_0}{2} \transpose{\betad} \betad \\
	 &= - \frac{1}{2} \transpose{\betad} \big[ \umat_d +  \xi_0 \mathrm{I}_K\big] \betad + \transpose{\betad} \eta_d \label{eq:ln_q_beta_vs}
\end{align}
Note that we have used the following result (equality written up to constants in $\betad$). 
\begin{equation}
	 E_{-q(\boldbeta)} \Bigg[ \frac{1}{2} \nsum \ksum -z_{nk} \tau_k \left( y_n - \transpose{x_n} \beta_k \right)^2 \Bigg] = - \frac{1}{2} \sum_d \transpose{\betad} \umat_d \betad + \sum_{d} \transpose{\betad} \eta_d
\end{equation}

From (\ref{eq:ln_q_beta_vs}), we see that
\begin{align*}
	q(\betad) = \mathcal{N} \left( \betad \given m_d, \qmat_d^{-1} \right)
\end{align*}

where $m_d = \qmat_d^{-1} \eta_d, \qmat_d = \umat_d + \xi_0 \mathrm{I}_K$, and $\eta_d = \zeta_d - \frac{1}{2} \sum\limits_{j \neq d}^D \mathrm{R}_{dj} \omega_j m_j$. We have made use of the following matrices
\begin{align*}
\mathrm{R}_{dj} & = \begin{bmatrix} 
\frac{a_1}{b_1} \nsum r_{n1} x_{nd}x_{nj}           &          &          \\ 
\vertdots   &  \ddots    & \vertdots \\
            &          & \frac{a_K}{b_K} \nsum r_{nK} x_{nd}x_{nj}        \\
\end{bmatrix} \quad \quad \quad \quad
\zeta_d =  \begin{bmatrix} 
\frac{a_1}{b_1} \nsum r_{n1} x_{nd}y_{n} \\ 
\vdots   \\
\frac{a_K}{b_K} \nsum r_{nK} x_{nd}y_{n} \\
\end{bmatrix} \\
\umat_d & = \begin{bmatrix} 
\frac{a_1}{b_1} \nsum r_{n1} x_{nd}^2            &               &  \\ 
            &  \ddots       &  \\
            &               & \frac{a_K}{b_K} \nsum r_{nK} x_{nd}^2 \\
\end{bmatrix}
\end{align*}


From (\ref{eq:ln_q_beta_vs}), we have
\begin{align*}
	q^{\star}(\betad) = \mathcal{N} \left( \betad \given m_d, \qmat_d^{-1} \right)
\end{align*}

where $m_d = \qmat_d^{-1} \eta_d, \qmat_d = \umat_d + \xi_0 \mathrm{I}_K$, and $\eta_d = \zeta_d - \frac{1}{2} \sum\limits_{j \neq d} \mathrm{R}_{dj} \omega_j m_j$. We have made use of the following matrices
\begin{alignat}{2}
	\umat_d &= \mathrm{diag} &&\left(\frac{a_1}{b_1} \nsum r_{n1} x_{nd}x_{nj}, \ldots, \frac{a_K}{b_K} \nsum r_{nK} x_{nd}x_{nj} \right) \\
	\mathrm{R}_{dj} &= \mathrm{diag} &&\left(\frac{a_1}{b_1} \nsum r_{n1} x_{nd}x_{nj}, \ldots, \frac{a_K}{b_K} \nsum r_{nK} x_{nd}x_{nj} \right) \\
	\zeta_d &= &&\transpose{\left( \frac{a_1}{b_1} \nsum r_{n1} x_{nd}y_{n}, \ldots, \frac{a_K}{b_K} \nsum r_{nK} x_{nd}y_{n} \right)}
\end{alignat}



Note that we restricted the approximating distribution for $\pr{\boldbeta, \boldsymbol\omega}$ to be of the form
$$ q \left( \boldbeta, \boldsymbol \omega\right) = \prod_{d} q \left( \betad, \omega_d \right) $$
where each of the $d$ factors then inherits a spike and slab density of the form
\[   
q \left( \betad, \omega_d \right) = 
\left\{
\begin{array}{ll}
      \lambda_d \mathcal{N} \left( \betad \given m_d, \qmat_d^{-1} \right),  & \textrm{ if } \omega_d = 1 \\
      (1 - \lambda_d) \delta_0 \left( \betad \right) & \textrm{ if } \omega_d = 0 \\
\end{array} 
\right. \]
The variational parameters $(m_d, \qmat_d, \lambda_d)$ can be updated using the following by differentiating the variational lower bound with respect to each of the parameters, setting the resulting partial derivatives to zero and solving for each parameter. Doing so yields following:
\setlength{\jot}{10pt}
\begin{alignat}{4}
  \mathrm{Cov}(\betad \given \omega_d = 1) \textrm{ } &\approx \textrm{ }&& \qmat_d^{-1} &&\textrm{ } & &= \textrm{ } \left(\umat_d + \xi_0 \mathrm{I}_K\right)^{-1} \label{eq:var_betad}\\
  \E [ \betad \given \omega_d = 1]  \textrm{ }&\approx \textrm{ } && m_d &&\textrm{ }& &= \textrm{ } \qmat_d^{-1} \eta_d  \label{eq:e_betad}\\
  \frac{\pr{\omega_d = 1\given \mathbf{X}, \mathbf{y}, \boldsymbol\theta}}{\pr{\omega_d = 0 \given \mathbf{X}, \mathbf{y}, \boldsymbol\theta}} \textrm{ } &\approx \textrm{ } && \frac{\lambda_d}{1-\lambda_d}&& \textrm{ }& &= \textrm{ } \frac{\pi}{1-\pi} \cdot \xi_0^{\frac{K}{2}} \cdot |\qmat_d|^{-\frac{1}{2}}\cdot \exp\Big\{\frac{1}{2} \cdot \transpose{m_d}\eta_d\Big\} \label{eq:logodds_lambda}
\end{alignat}



\newpage

\subsection{Evidence Lower Bound (ELBO)}
In order to evaluate the convergence of the coordinate ascent algorithm, we can calculate the evidence lower bound using the updated variational parameters at the end of each iteration. Since the ELBO is monotonic increasing, we continue the coordinate ascent until the change in the ELBO between iterations falls below a predetermined tolerance. Note that the expectations taken below are with respect to the optimal variational distributions defined in the previous section.

\begin{equation} \label{eq:ELBO}
\setlength{\jot}{11pt}
\begin{split}
	\mathcal{L}(q) &= \sum_{z} \iiiint q\left(\mathbf{Z}, \boldtau, \boldbeta, \boldgamma, \boldsymbol \omega\right)
	\ln \Bigg\{ \frac{p\left(\mathbf{y}, \mathbf{X}, \boldbeta, \boldsymbol\omega, \boldtau, \mathbf{Z}\right)}{q \left(\mathbf{Z}, \boldtau, \boldbeta, \boldgamma, \boldsymbol \omega \right)}\Bigg\}\, d\boldbeta\, d\boldsymbol\omega\, d\boldtau\, d\boldgamma \\
	&= \E_q \ln p\left( \mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z} \right) + \E_q \ln p\left( \mathbf{Z} \given \mathbf{X}, \boldgamma \right) + \E_q \ln p\left( \boldgamma \right) + \E_q \ln p\left( \boldtau \right) \\ & \quad + \E_q \ln p\left( \boldbeta, \boldomega \right) - \E_q \ln q\left( \boldbeta, \boldomega \right) - 
	\E_q \ln q\left( \boldgamma \right) - \E_q \ln q\left( \mathbf{Z} \right) - \E_q \ln q\left( \mathbf{\tau} \right)
\end{split}
\end{equation}



\begin{align*}
	& \E_q \ln p\left( \mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z} \right) = - \frac{1}{2} \nsum \ksum r_{nk} \Big\{  \ln(2\pi) - (\psi(a_k) - \psi(b_k)) + \frac{a_k}{b_k} \cdot y_n^2 \Big\} \\ & \qquad \qquad \qquad\qquad \qquad - \frac{1}{2} \cdot \sum_d \lambda_d \left( \transpose{m_d} U_d m_d + \trace{\mathrm{U}_d \mathrm{Q}_d^{-1}} - 2\transpose{m_d} \Bigg[ \zeta_d - \frac{1}{2} \sum_{j\neq d} \lambda_j \mathrm{R}_{dj} m_j\Bigg] \right)\\
	& \E_q \ln p\left( \mathbf{Z} \given \mathbf{X}, \boldgamma \right)  = \nsum \ksum r_{nk} \Big\{ \transpose{x_n} \mu_k - \alpha_n - \varphi_n \Big\}\\
	& \E_q \ln p\left( \boldgamma \right) = -\frac{KD}{2} \ln(2\pi) - \frac{1}{2} \left( \transpose{\mu_k} \mu_k + \trace{\mathrm{V}_k^{-1}} \right) \\
	& \E_q \ln p\left( \boldtau \right) = K \left( a_0 \ln b_0 - \ln \Gamma(a_0) \right) - \ksum b_0 \cdot \frac{a_k}{b_k} + (a_0 - 1) \left( \psi(a_k) - \psi(b_k) \right) \\
	& \E_q \ln p\left( \boldbeta, \boldomega \right) = \sum_d \lambda_d \ln \pi + (1 - \lambda_d) \ln (1 - \pi) + \frac{\lambda_d}{2} \cdot \Big\{ K \xi_0 - K \ln (2 \pi) - \xi_0 \left( \trace{\qmat_d^{-1}} + \transpose{m_d}m_d \right) \Big\} \\
	& \E_q \ln q\left( \boldbeta, \boldomega \right) = \sum_d \lambda_d \ln \pi + (1 - \lambda_d) \ln (1 - \pi) + \frac{\lambda_d}{2} \cdot \Big\{ K \xi_0 - K \ln (2 \pi) - \xi_0 \left( \trace{\qmat_d^{-1}} + \transpose{m_d}m_d \right) \Big\}\\
	& \E_q \ln q\left( \boldgamma \right) = - \frac{KD}{2} \left( \ln(2\pi) + 1 \right) + \frac{1}{2} \cdot \ksum \mathrm{V}_k \\
	& \E_q \ln q\left( \mathbf{Z} \right) = \nsum \ksum r_{nk} \ln r_{nk} \\
	& \E_q \ln q\left( \mathbf{\tau} \right) = \ksum a_k \left( \ln b_k - 1 \right) - \ln \Gamma(a_k)  + (a_k - 1) \cdot \left( \psi(a_k) - \psi(b_k) \right)
\end{align*}





\newpage

\appendix
%%%%% details for updating q(Z) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variational Update for $q(\mathbf{Z})$} 
\label{app:q_z}

Taking the expectation with respect to the other variational parameters, we can derive the following variational distribution for $\mathbf{Z}$,
\begin{align*}
	\ln q^{*}(\mathbf{Z}) &= \sum_n \sum_k z_{nk} \Bigg\{  -\frac{1}{2}\ln(2\pi) + \frac{1}{2} \E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} \E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2] \\ 
	&\qquad \qquad \qquad \quad + x_n^{\tr}\E_{q(\boldsymbol\gamma)}[\gamma_k] - \E_{q(\boldsymbol\gamma)}\Bigg[\ln \sum_{j} \exp \{ x_n^{\tr} \gamma_j \}\Bigg]\Bigg\} \\
	&= \sum_n \sum_k z_{nk} \ln \rho_{nk}
\end{align*}

where we have defined 
\begin{equation} \label{eq:ln_rho}
\begin{split}
 \ln \rho_{nk} = &-\frac{1}{2}\ln(2\pi) + \frac{1}{2} \E_{q(\boldsymbol\tau)}[ \ln \tau_k ] - \frac{1}{2} \E_{q(\boldsymbol\beta, \boldsymbol\tau)}[\tau_k (y_n - x_n^{\tr}\beta_k)^2] \\ 
	& + x_n^{\tr}\E_{q(\boldsymbol\gamma)}[\gamma_k] - \E_{q(\boldsymbol\gamma)}\Bigg[\ln \sum_{j} \exp \{ x_n^{\tr} \gamma_j \} \Bigg]
\end{split}
\end{equation}


Exponentiating and normalizing, we have
\begin{equation} \label{eq:q_z}
	q^{*}(\mathbf{Z}) = \prod_{n} \prod_{k} r_{nk}^{z_{nk}}, \quad r_{nk} = \frac{\rho_{nk}}{\sum_{j} \rho_{nj}}
\end{equation}

For the discrete distribution $q^{*}(\mathbf{Z})$ given in (\ref{eq:q_z}) above, we have $E[z_{nk}] = r_{nk}$. Note, however, that in order to compute the expectation in closed form, we need an expression for the four expectations involved in the quantity $\ln \rho_{nk}$, as defined in (\ref{eq:ln_rho}). \\

From the results derived in Appendix \ref{app:beta_tau}, we know that $q^{*}(\tau_k) = \mathrm{Ga}(\tau_k \given a_k, b_k)$. We can then compute the following expectation with respect to $q^{*}(\boldsymbol\tau))$.

\begin{equation} \label{eq:exp_lntau}
	E_{q(\boldsymbol\tau)}[ \ln \tau_k ] &= \psi(a_k) - \psi(b_k)
\end{equation}

Again from Appendix \ref{app:beta_tau}, we can then compute the following expectation with respect to $q^{*}(\beta_k, \tau_k)$.
\begin{equation} \label{eq:exp_taubeta}
\begin{split}
	\E_{q(\boldsymbol\beta, \boldsymbol\tau)}\big[\tau_k (y_n - x_n^{\tr}\beta_k)^2\big] &= 
	\E \bigg[\tau_k \left( y_n + m_k^{\tr} x_n x_n^{\tr} m_k + \mathrm{tr} \left(x_n x_n^{\tr}\left(\tau_k \mathrm{Q}_k \right)^{-1} \right) - 2y_n x_n^{\tr} m_k \right) \bigg] \\
	&=  \frac{a_k}{b_k} \left(y_n^2 + m_k^{\tr}x_nx_n^{\tr} m_k -  2y_n x_n^{\tr} m_k\right) + \mathrm{tr} \left( x_n x_n^{\tr} \mathrm{Q}_k^{-1}\right) \\
	&= \frac{a_k}{b_k}(y_n - m_k^{\tr}x_n)^2 + x_n^{\tr} \mathrm{Q}_k^{-1} x_n
\end{split}
\end{equation}


From the expression derived in (\ref{eq:gamma_params}) of Appendix \ref{app:gamma}, we have $q^{*}(\gamma_k) = \mathcal{N}(\gamma_k \given \mu_k, \mathrm{V}_k^{-1})$, then we have
\begin{equation} \label{eq:exp_gamma}
	\E_{q(\gamma_k)}[\gamma_k] = \mu_k
\end{equation}

Using the bound discussed in Appendix \ref{app:gamma}, equation (\ref{eq:jj_bound}), we can then compute the following expectation with respect to $q^{*}(\boldsymbol\gamma)$.
\begin{equation} \label{eq:exp_lse_gamma}
\begin{split}
	& \E_{q(\boldsymbol\gamma)} \Bigg[ \ln \sum_{j}^K \exp \{ x_n^{\tr} \gamma_j \} \Bigg] \\
	& \approx \E_{q(\boldsymbol\gamma)} \Bigg[ \alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n - \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right) \Bigg] \\
	& = \alpha_n + \sum_{j}^K \frac{1}{2}\left(x_n^{\tr}\mu_j - \alpha_n - \xi_{nj}\right) + \lambda(\xi_{nj}) \left( (x_n^{\tr} \mu_j - \alpha_n)^2 - \xi_{nj}^2 + x_n^{\tr} \mathrm{V}_j^{-1} x_n \right) + \log( 1 + e^{\xi_{nj}}) \\
	&= \alpha_n + \sum_{j}^K \frac{1}{2}\left(x_n^{\tr}\mu_j - \alpha_n - \xi_{nj}\right) + \log( 1 + e^{\xi_{nj}})
\end{split}
\end{equation}
since $(x_n^{\tr} \mu_j - \alpha_n)^2 - \xi_{nj}^2 = - x_n^{\tr} \mathrm{V}_j^{-1} x_n$. Gathering the results in (\ref{eq:exp_lntau}), (\ref{eq:exp_taubeta}), (\ref{eq:exp_gamma}), and (\ref{eq:exp_lse_gamma}), and substituting these into (\ref{eq:ln_rho}), we can compute $\E[z_{nk}] = r_{nk}$ in closed form. 


% \newpage

%%%%% details for updating q(Z) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variational Updates for $q(\gamma_k)$} \label{app:gamma}  
% contains details for using the Bouchard bound

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

For the variational distribution for $\gamma_k$, we first note the following bound \parencite{bouchard:07}, $\sum_{j = 1}^{K} e^{t_j} \leq \prod_{j = 1}^K (1 + e^{t_j})$. Setting $t_j = x_n^{^\intercal} \gamma_j - \alpha_{n}$ and then taking log, we have the following bound:
\begin{equation} \label{eq:log-sum-exp}
	\log \left( \sum_{j = 1}^K \exp\{ x_n^{^\intercal} \gamma_j \}\right) \leq \alpha_n + \sum_{j=1}^K \log \left( 1 + \exp \{ x_n^{^\intercal} \gamma_j - \alpha_n \}\right)
\end{equation}


We can further bound this by using the following tangential bound \parencite{jj:2001}, $$\log(1 + e^x) \leq \frac{x - \xi}{2} + \frac{1}{4\xi} \tanh \left( \frac{\xi}{2} \right) (x^2 - \xi^2) + \log\left(1 + e^{\xi}\right)$$ then we arrive at the following bound: 
\begin{equation} \label{eq:jj_bound}
	\log \left( \sum_{j = 1}^K \exp\{ x_n^{^\intercal} \gamma_j \}\right) \leq 
\alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n - \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right)
\end{equation}

where $\lambda(\xi) = \frac{1}{4\xi} \tanh \left( \frac{\xi}{2} \right)$. Then we can substitute this back into $\ln q^{*}(\gamma_k)$ to obtain an approximation for the left hand side of (\ref{eq:log-sum-exp}), thus allowing us to obtain a closed form for the variational distribution. Note that all of the equalities above are written up to constants.
\begin{align*}
    \ln q^{*}(\gamma_k) &= - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \sum_{n} r_{nk} x_n^{\intercal} \gamma_k  - \sum_n r_{nk} \ln \left( \sum_j \exp\{x_n^{\intercal} \gamma_j \} \right)  \\
    & \approx - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \gamma_k^{\intercal} \sum_{n} r_{nk} x_n \\
    & - \sum_n r_{nk} \Bigg\{ \alpha_n + \sum_{j = 1}^K \frac{x_n^{\intercal} \gamma_j - \alpha_n - \xi_{nj}}{2} + \lambda(\xi_{nj}) \left( (x_n^{\intercal} \gamma_j - \alpha_n)^2 - \xi_{nj}^2\right) + \log \left( 1 + e^{\xi_{nj}}\right) \Bigg\} \\
    & = - \frac{1}{2} \gamma_k^{\tr} \gamma_k + \gamma_k^{\intercal} \sum_{n} r_{nk} x_n - \sum_n r_{nk} \Bigg\{ \frac{1}{2} \gamma_k^{\tr} x_n + \lambda\left( \xi_{nj} \right) \left( \gamma_k^{\tr}x_n x_n^{\tr} \gamma_k - 2\alpha_n \gamma_k^{\tr} x_n \right)\Bigg\} \\
    &= -\frac{1}{2} \gamma_k^{\tr} \left(\eye_D  + 2 \sum_n r_{nk} \lambda(\xi_{nk}) x_n x_n^{\tr} \right) \gamma_k + \gamma_k' \left( \sum_n r_{nk} \left(\frac{1}{2} + 2 \lambda \left( \xi_{nk}\right) \alpha_n  x_n\right) \right)
\end{align*}
Exponentiating, we can recover $q^{*}(\gamma_k) = \mathcal{N} \left(\gamma_k \given \mu_k, \mathrm{V}_k^{-1} \right)$, where
\begin{equation} \label{eq:gamma_params}
\begin{split}
	& \mu_k = \mathrm{V}_k^{-1} \eta_k \\
	& \eta_k = \sum_{n} r_{nk} \left( \frac{1}{2} + 2 \lambda(\xi_{nk}) \alpha_n \right) x_n \\
	& \mathrm{V}_k = \eye_D + 2 \sum_{n} r_{nk} \lambda(\xi_{nk}) x_n x_n^{\tr}
\end{split}
\end{equation}

The additional parameters introduced in the two upper bounds can be updated using the following equations \parencite{Depraetere:17},
\begin{align*}
    \xi_{nk} & = \sqrt{\left(\mu_k^{\intercal}x_n - \alpha_n \right)^2 + x_n^{\intercal} \mathrm{V}_k^{-1} x_n} \qquad \qquad \forall k, n \\ \\
    \alpha_n & = \frac{\frac{1}{2}\left( \frac{K}{2} - 1\right) + \sum_{j = 1}^K \lambda \left( \xi_{nj} \right)\mu_j^{\intercal} x_n}{\sum_{j=1}^{K} \lambda \left( \xi_{nj}\right)} \qquad \forall n
\end{align*}


%%%%% details for updating q(beta, tau) %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variational Updates for $q(\beta_k, \tau_k)$} \label{app:beta_tau}  
Using results from Appendix \ref{app:q_z}, we can write the following expression for the joint variational distribution of $(\beta_k, \tau_k)$, 
\begin{equation} \label{eq:lnq_beta_tau}
\begin{split}
	\ln q^{*}(\beta_k, \tau_k) = & \sum_{n} -\frac{1}{2} r_{nk} \tau_k \left( y_n^2 + \transpose{\beta_k} x_n \transpose{x_n} \beta_k - 2y_n \transpose{\beta_k} x_n \right) + \frac{r_{nk}}{2} \ln \tau_k + \frac{D}{2} \ln \tau_k \\
	& - \frac{\tau_k}{2} \left( \transpose{\beta_k} \Lambda_0 \beta_k + \transpose{m_0} \Lambda_0 m_0 - 2\transpose{\beta_k}\Lambda_0m_0\right) + (a_0 - 1) \ln \tau_k - b_0 \tau_k
\end{split}
\end{equation}

We first consider terms on the right hand side of (\ref{eq:lnq_beta_tau}) that depend on $\beta_k$ to find $\ln q^{\star}(\beta_k \given \tau_k)$, giving
\begin{equation} \label{eq:lnq_beta}
	\ln q^{\star}(\beta_k \given \tau_k) = -\frac{\tau_k}{2} \transpose{\beta_k} \Big[ \sum_{n}r_{nk} x_n \transpose{x_n} + \Lambda_0 \Big] \beta_k + \tau_k \transpose{\beta_k} \Big[ \sum_{n} r_{nk}y_n x_n + \Lambda_0 m_0 \Big]
\end{equation}
\begin{equation} \label{q_beta}
	q^{*}(\beta_k \given \tau_k) = \mathcal{N}\left(\beta_k \given m_k, (\tau_k \mathrm{Q}_k)^{-1} \right)
\end{equation}
\begin{equation} \label{eq:beta_params}
\begin{split}
    & \mathrm{Q}_k = \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 \\
 	& \zeta_k = \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0 \\
	& m_k = \mathrm{Q}_k^{-1}  \zeta_k 
\end{split}
\end{equation}

Then we can make use of the relation $\ln q^{*}(\tau_k) = \ln q^{\star}(\beta_k, \tau_k) -  \ln q^{\star}(\beta_k \given \tau_k)$, where the quantities on the right hand side come from (\ref{eq:lnq_beta_tau}) and (\ref{q_beta}). Note that equality below is written up to constants, keeping only terms involving $\tau_k$. 
\begin{equation}\label{eq:ln_q_tau}
\begin{split}
	 \ln q^{*}(\tau_k) = (a_0 + N_k - 1) \ln \tau_k - \tau_k \Bigg\{ b_0 & + \frac{1}{2} \left( \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{m_k} \mathrm{Q}_k m_k \right) \\
	 & + \frac{1}{2} \transpose{\beta_k} \left( \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 - \mathrm{Q}_k \right) \beta_k \\
	 & - 2 \transpose{\beta_k} \left( \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0 - \mathrm{Q}_k m_k \right) \Bigg\}
\end{split}
\end{equation}
Exponentiating, we arrive at the following distribution
\begin{equation} \label{eq:q_tau}
	q^{*}(\tau_k) =  \mathrm{Ga}\left( \tau_k \given a_k, b_k \right)
\end{equation}
where we have defined
\begin{equation} \label{eq:tau_params}
\begin{split}
	& a_k = a_0 + \frac{N_k}{2} \\
	& b_k = b_0 + \frac{1}{2} \sum_{n} r_{nk} y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{\zeta_k} \mathrm{Q}_k^{-1} \zeta_k
\end{split}
\end{equation}

The expression for $b_k$ arises by noting that the three following simplifications for the summation terms in the coefficient of $\tau_k$ in (\ref{eq:ln_q_tau}), 
\begin{align*}
	& \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 - \transpose{m_k} \mathrm{Q}_k m_k =  \sum_{n}r_{nk}y_n^2 + \transpose{m_0} \Lambda_0 m_0 -\transpose{\zeta_k} \mathrm{Q}_k^{-1} \zeta_k \\
	& \sum_{n} r_{nk} x_n \transpose{x_n} + \Lambda_0 - \mathrm{Q}_k = 0 \\
	& \sum_{n} r_{nk} y_n x_n + \Lambda_0 m_0 - \mathrm{Q}_k m_k = 0
\end{align*}

where the first equality holds by expanding $\transpose{m_k} \mathrm{Q}_k m_k = \transpose{\zeta_k} \left( \mathrm{Q}_k^{-1}\right)^{\intercal} \mathrm{Q}_k \mathrm{Q}_k^{-1} \zeta_k = \transpose{b_k} \mathrm{Q}_k^{-1} \zeta_k$. The second quality holds by recalling the definition of $\mathrm{Q}_k$ in (\ref{eq:beta_params}), and the third equality holds by observing from (\ref{eq:beta_params}) that $\mathrm{Q}_{k} m_k = \zeta_k$

\section{Variational Lower Bound} \label{app:elbo}  %% EBLO DERIVATION
As seen in (\ref{eq:ELBO}) of section \ref{sub:elbo}, we need to calculate seven expectations (taken with respect to the variational distribution $q(\mathbf{Z}, \boldbeta, \boldtau, \boldgamma)$. Below, we compute each of these expectations in detail, making extensive use of the variational distributions derived in Appendix \ref{app:q_z}, \ref{app:gamma}, and \ref{app:beta_tau}. 
% log marginal likelihood: ln p ( y | X, beta, tau, Z)
\begin{equation} \label{eq:e1_deriv}
\begin{split}
	\E[\ln\pr{\mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}}] &= \nsum \ksum \E[z_{nk}] \E \Bigg[ \ln \mathcal{N}\left(y_n \given \transpose{x_n} \beta_k, \tau_k^{-1}\right)\Bigg]  \\
	&= -\frac{1}{2} \nsum \ksum r_{nk} \Big\{ \ln(2 \pi) - \E[\ln \tau_k] + \E\Big[\tau_k \left( y_n - \transpose{x_n} \beta_k\right)^2\Big]\Big\} \\
	&= -\frac{1}{2}\nsum \ksum r_{nk} \Big\{ \ln (2\pi) - (\psi(a_k) - \psi(b_k)) + \transpose{x_n}\mathrm{Q}_k^{-1}x_n \\ & \qquad \qquad + \frac{a_k}{b_k}(y_n - \transpose{x_n}m_k)^2 \Big\}
\end{split}
\end{equation}

\begin{equation} \label{eq:e2_deriv}
\begin{split}
	\E[\ln \pr{\mathbf{Z} \given \mathbf{X}, \boldgamma}] &= \nsum \ksum \E[z_{nk}] \left( \E[\transpose{x_n} \gamma_k] - \E\Big[\ln \sum_{j} \exp \{ \transpose{x_n}\gamma_j\}\Big]\right)\\
	&\approx \nsum \ksum r_{nk} \left( \transpose{x_n}\mu_k - \alpha_n - \varphi_n \right)
\end{split}
\end{equation}
where $\varphi_n = \sum\limits_{j=1}^K \frac{1}{2}\left(x_n^{\tr}\mu_j - \alpha_n - \xi_{nj}\right) + \log( 1 + e^{\xi_{nj}}).$
Here, we make use of the result in (\ref{eq:exp_lse_gamma}), where we take the expectation of the upper bound previously derived. 
\begin{equation} \label{eq:e3_deriv}
\begin{split}
	\E[\ln \pr{\boldgamma}] &= \ksum \E \Big[ \ln \mathcal{N}\left( \gamma_k \given 0, \eye_D\right)\Big] \\
	&= -\frac{K \cdot D}{2} \ln (2\pi) - \frac{1}{2} \ksum \transpose{\mu_k}\mu_k + \mathrm{tr} \left( \mathrm{V}_k^{-1} \right)
\end{split}
\end{equation}
\begin{equation} \label{eq:e4_deriv} % E [ ln p(beta, tau) ]
\begin{split}
	\E[\ln \pr{\boldbeta, \boldtau}] &= \ksum \E\Big[\ln \mathcal{N}\left( \beta_k \given m_0, \left(\tau_k \Lambda_0 \right)^{-1} \right)\Big] + \E\Big[\ln \mathrm{Ga}\left( \tau_k \given a_0, b_0\right)\Big]\\
	&= \left( a_0 + \frac{D}{2} - 1 \right) \ksum \psi(a_k) - \psi(b_k)  \\
	& \qquad - \frac{1}{2}\ksum \Bigg\{ \frac{a_k}{b_k}\Big[\transpose{(m_k - m_0)} \Lambda_0 (m_k - m_0) + b_0 \Big] + \mathrm{tr} \left( \Lambda_0 \mathrm{Q}_{k}^{-1}\right) \Bigg\} \\
	& \qquad - K \left(\frac{D}{2}\ln(2\pi)  - \frac{1}{2}\ln |\Lambda_0| - a_0 \ln b_0 + \ln \Gamma(\alpha_0) \right)
\end{split}
\end{equation}
where we make use of $\E[\tau_k\transpose{(\beta_k - m_0) \Lambda_0 (\beta_k - m_0)}] =  \frac{a_k}{b_k} \transpose{(m_k - m_0)} \Lambda_0 (m_k - m_0) + \mathrm{tr} \left( \Lambda_0 \mathrm{V}_{k}^{-1}\right)$. The other expectations can be calculated using results derived in Appendix \ref{app:q_z}. In the following expectation, we make use of the established result that $E[z_{nk}] = r_{nk}$. 
\begin{equation} \label{eq:e5_deriv}
\begin{split}
	\E[\ln q(\mathbf{Z})] &=  \nsum \ksum r_{nk} \ln r_{nk}
\end{split}
\end{equation}

\begin{equation} \label{eq:e6_deriv} % E [ln q(beta, tau)]
\begin{split}
	\E[\ln q(\boldbeta, \boldtau)] &= \ksum \E \Big[ \ln \mathcal{N} \left( \beta_k \given m_k, (\tau_k \mathrm{Q}_k)^{-1}\right)\Big] + \E\Big[\ln \mathrm{Ga} \left( \tau_k \given a_k, b_k \right)\Big] \\
	&= \ksum \left( \frac{D}{2} + a_k - 1 \right) \big[\psi(a_k) - \psi(b_k) \big] + a_k \ln b_k - a_k - \ln \Gamma(a_k) + \frac{1}{2}\ln |\mathrm{Q}_{k}|\\
	& \quad \quad \quad -\frac{KD}{2} \left(  \ln(2\pi)  + 1\right)
\end{split}
\end{equation}
where we make use of $\E[\tau_k \transpose{(\beta_k - m_k)}\mathrm{Q}_k(\beta_k - m_k)] = D$. The other expectations can be calculated using results derived in Appendix \ref{app:q_z}.

\begin{equation} \label{eq:e7_deriv}
\begin{split}
	\E[\ln q(\boldgamma)] &= \ksum \E \Big[ \mathcal{N} \left( \gamma_k \given \mu_k, \mathrm{V}_{k}^{-1} \right)\Big] \\
	&= - \frac{KD}{2}(\ln (2\pi) + 1) + \frac{1}{2}\sum_{k} \ln|\mathrm{V}_k| 
\end{split}
\end{equation}
since $\E[\transpose{(\gamma_k - \mu_k)}\mathrm{V}_{k}(\gamma_k - \mu_k)] = D$.


\newpage


\begin{figure}
  \centering
  \tikz{ %
    \node[latent] (gamma) {$\gamma$} ; %
    \node[latent, right=of gamma] (z) {$z_n$} ; %
    \node[latent, above=of z] (tau) {$\tau$} ; %
    \node[obs, right=of z] (yn) {$y_n$} ; %
    \node[det, below =of yn] (xn) {$x_n$};
    \node[latent, above=of yn] (beta) {$\beta$} ; %
    \plate[inner sep=0.25cm, xshift=-0.12cm, yshift=0.12cm] {plate1} {(z) (yn) (xn)} {N}; %
    \edge {gamma} {z} ; %
    \edge {z,tau,beta} {yn} ; %
    \edge {beta} {yn} ; %
    \edge {tau} {beta} ; %
    \edge {xn} {yn}
    \edge {xn} {z}
  }
\end{figure}

\subsection{Commonly Used Expectations}

Two useful formulations of the approximating density: $q \left( \betad, \omega_d \right)$

\begin{equation} \label{eq:qbeta_omega}
	q(\betad \given \omega_d) = \omega_d \qbeta + (1 - \omega_d) \delta_0(\betad)
\end{equation}
\begin{equation}  \label{eq:qbeta_marginal}
	q(\betad) = \lambda_d \qbeta + (1 - \lambda_d) \delta_0(\betad)
\end{equation}

\begin{align*}
	\E \Big[ \betad \Big] &= \lambda_d m_d \\
	\E \Big[ \betad \given \omega_d \Big] &=  \omega_d m_d \\
	\E \Big[ \betad \transpose{\betad} \given \omega_d \Big] &= \omega_d \left( \qmat_d^{-1} + m_d \transpose{m_d} \right) \\
	\var{\betad \given \omega_d} &= \omega_d \qmat_d^{-1} + \omega_d(1-\omega_d) m_d \transpose{m_d} \\
	\E\Big[ \betad - m_d \given \omega_d \Big] &= (\omega_d - 1) m_d \\
	\E \Big[ \transpose{\betad} \betad \Big] &= \lambda_d \left( \trace{\qmat_d^{-1}} + \transpose{m_d} m_d \right) \\
	\E \Big[ \transpose{\betad} \umat_d \betad \Big] &= \lambda_d \trace{\umat_d \qmat_d^{-1}} + \lambda_d \transpose{m_d} \umat_d m_d \\
	\E \Big[ \transpose{(\betad - m_d)} \qmat_{d}(\betad - m_d) \given \omega_d \Big] &= K \cdot \omega_d + \transpose{m_d}\qmat_d m_d - \omega_d \transpose{m_d}\qmat_d m_d\\
	\E \Big[ \omega_d \transpose{(\betad - m_d)} \qmat_{d}(\betad - m_d) \Big] &= K \cdot \lambda_d \\
	\E \Big[ \betad \transpose{\betad} \Big] &=  \lambda_d \left( \qmat_d^{-1} + m_d \transpose{m_d} \right) \\
	\var{\betad} &= \lambda_d \qmat_d^{-1} + \lambda_d(1 - \lambda_d) m_d \transpose{m_d}
\end{align*}

\subsection{Expectation Calculations Details}

\begin{align*}
	\E \Big[ \betad \Big] &= \int \betad \cdot  \lambda_d \cdot \qbeta d\beta_d = \lambda_d m_d \\
	\E \Big[ \betad \given \omega_d \Big] &= \int  \betad \cdot \omega_d \cdot \qbeta d \betad = \omega_d m_d \\
	\E \Big[ \transpose{\betad} \betad \given \omega_d \Big] &= \transpose{\E \Big[ \betad \given \omega_d \Big]} \E \Big[ \transpose{\betad} \given \omega_d \Big] + \trace{\var{\betad \given \omega_d}} \\
	&= \omega_d^2 \transpose{m_d} m_d + \trace{\omega_d \qmat_d^{-1} + \omega_d(1 - \omega_d) m_d \transpose{m_d}} \\
	&= \omega_d \left( \trace{\qmat_d^{-1}} + m_d \transpose{m_d} \right) \\
	\E \Big[ \omega_d \transpose{\betad} \betad \Big] &= \E \Big[ \omega_d \cdot \E \left( \transpose{\betad} \betad \given \omega_d\right) \Big] \\
	&= \E \Big[ \omega_d^2 \left( \trace{\qmat_d^{-1}} + \transpose{m_d} m_d \right)\Big] \\
	&= \lambda_d \left( \trace{\qmat_d^{-1}} + \transpose{m_d} m_d\right) \\
	\E \Big[ \transpose{\betad} \betad\Big] &= \E \Big[ \E \left( \transpose{\betad} \betad \given \omega_d\right)\Big] \\
	&= \E \Big[ \omega_d \left( \trace{\qmat_d^{-1}} + m_d \transpose{m_d} \right) \Big]\\
	&= \lambda_d \left( \trace{\qmat_d^{-1}} + m_d \transpose{m_d} \right)\\
	\E \Big[ \betad \transpose{\betad} \given \omega_d \Big] &= \int \betad \transpose{\betad} \cdot \omega_d \qbeta d\betad  = \omega_d \left( \qmat_d^{-1} + m_d \transpose{m_d} \right) \\
	\var{\betad \given \omega_d} &= \E\Big[  (\betad - \E(\betad))\transpose{(\betad - \E(\betad))} \given \omega_d \Big] \\
	&= \E\Big[\betad \transpose{\betad} \given \omega_d \Big] + \E \Big[ \betad \given \omega_d \Big] \transpose{\E \Big[ \betad \given \omega_d \Big]} - 2 \E \Big[ \betad \given \omega_d \Big] \transpose{\E \Big[ \betad \given \omega_d \Big]} \\
	&= \omega_d \left( m_d \transpose{m_d} + \qmat_d^{-1} \right) + \omega_d^2 m_d \transpose{m_d} - 2\omega_d^2 m_d\transpose{m_d} \\
	&= \omega_d \qmat_d^{-1} + \omega_d(1-\omega_d) m_d \transpose{m_d} \\
	\E \Big[ \transpose{\betad} \umat_d \betad \Big] &= \transpose{\E(\betad)} \umat_d \E(\betad) + \trace{\umat_d \var{\betad}} \\
	&= \lambda_d^2 \transpose{m_d} \umat_d m_d + \trace{\umat_d \left( \lambda_d \qmat_d^{-1} + \lambda_d(1-\lambda_d)m_d \transpose{m_d} \right)} \\
	&= \lambda_d^2 \transpose{m_d} \umat_d m_d + \lambda_d \trace{\umat_d\qmat_d^{-1}} + \lambda_d(1 - \lambda_d)\transpose{m_d}\umat_dm_d \\
	&= \lambda_d \trace{\umat_d \qmat_d^{-1}} + \lambda_d \transpose{m_d} \umat_d m_d
\end{align*}



\begin{align*}
	\E\Big[ \betad - m_d \given \omega_d \Big] &= \omega_d m_d - m_d = (\omega_d - 1) m_d \\
	\E \Big[ \transpose{(\betad - m_d)} \qmat_{d}(\betad - m_d) \given \omega_d \Big] &= \transpose{\E \Big[ \transpose{(\betad - m_d)} \given \omega_d \Big]} \qmat_d \E \Big[ \transpose{(\betad - m_d)} \given \omega_d \Big] + \trace{\qmat_d \var{\betad \given \omega_d}} \\
	&= (\omega_d-1)^2 \transpose{m_d} \qmat_d m_d + \omega_d \trace{\qmat_d \qmat_d^{-1}} + \omega_d(1-\omega_d)\transpose{m_d}\qmat_d m_d \\
	&= (\omega_d^2 - 2\omega_d + 1)\transpose{m_d} \qmat_d m_d + \omega_d \trace{\mathrm{I}_K} + \omega_d\transpose{m_d}\qmat_d m_d -\omega_d^2\transpose{m_d}\qmat_d m_d \\
	&= K \cdot \omega_d + \transpose{m_d}\qmat_d m_d - \omega_d \transpose{m_d}\qmat_d m_d \\
	\E \Big[ \omega_d \transpose{(\betad - m_d)} \qmat_{d}(\betad - m_d) \Big] &= \E \Big[ \omega_d \E \left(\transpose{(\betad - m_d)} \qmat_{d}(\betad - m_d) \given \omega_d \right) \Big]\\
	&= \E \Big[ \omega_d \left( K \cdot \omega_d + \transpose{m_d}\qmat_d m_d - \omega_d \transpose{m_d}\qmat_d m_d\right) \Big] \\
	&= \E \Big[ K \cdot \omegad^2 + \omega_d \transpose{m_d}\qmat_d m_d - \omega_d^2 \transpose{m_d}\qmat_d m_d \Big] \\
	&= K \cdot \lambda_d + \lambda_d \transpose{m_d}\qmat_d m_d - \lambda_d \transpose{m_d}\qmat_d m_d \\
	&= K \cdot \lambda_d \\
	\E \Big[ \betad \transpose{\betad} \Big] &= \int \betad \transpose{\betad} \cdot  \lambda_d \cdot 
\qbeta d\beta_d = \lambda_d \left( \qmat_d^{-1} + m_d \transpose{m_d} \right) \\
	\var{\betad} &= \E \Big[ \betad \transpose{\betad} \Big] - \E \Big[ \betad\Big] \transpose{\E \Big[ \betad\Big]} \\
	&= \lambda_d \left( \qmat_d^{-1} + m_d \transpose{m_d} \right) - \lambda_d^2 m_d \transpose{m_d} \\
	&= \lambda_d \qmat_d^{-1} + \lambda_d(1 - \lambda_d) m_d \transpose{m_d}
\end{align*}

\newpage


\section{Variational Update for $q(\betad \given \omega_d = 1)$}

Equality in first two sets of equations is written to constants in $\betad$. The last three sets of equations provide the details in the first expectation on the RHS of the first equation. 

\begin{align*}
	 \ln q\left(\betad \given \omega_d = 1 \right) &= - \frac{1}{2}\E \Big[ \nsum \ksum z_{nk} \tau_k \left( y_n - \transpose{x_n} \beta_k \right)^2\Big] - \frac{\xi_0}{2} \transpose{\betad} \betad \\
	 &= - \frac{1}{2} \transpose{\betad} \mathrm{U}_d \betad - \frac{1}{2} \transpose{\betad} \sum_{j \neq d} \mathrm{R}_{dj} \E \big[ \tilde{\beta}_j \big] + \transpose{\betad} \zeta_d - \frac{\xi_0}{2}  \transpose{\betad} \betad \\
	 &= - \frac{1}{2} \transpose{\betad} \big[ \umat_d +  \xi_0 \mathrm{I}_K\big] \betad + \transpose{\betad} \eta_d 
\end{align*}


\begin{align*}
	\E \Big[ \nsum \ksum z_{nk} \tau_k \left( y_n - \transpose{x_n} \beta_k \right)^2 \Big]  &= \nsum \ksum r_{nk} \cdot \frac{a_k}{b_k} \cdot \E \left( y_n^2 + \transpose{\beta_k} x_n \transpose{x_n} \beta_k - 2y_n \transpose{\beta_k}x_n \right) \\
	&=  \sum_d \transpose{\betad} \mathrm{U}_d \betad + \sum_d \transpose{\betad} \sum_{j \neq d} \mathrm{R}_{dj} \E \big[ \tilde{\beta}_j \big] - 2 \sum_d \transpose{\betad} \zeta_d
\end{align*}

Note that the first equality decomposes into three terms, the first of which we can drop because it is not a function of $\betad$. The second and third terms are computed in detail below to give us second equality. 

\begin{align*}
	\nsum \ksum r_{nk} \cdot \frac{a_k}{b_k} \cdot \E \left( \transpose{\beta_k} x_n \transpose{x_n} \beta_k \right) &= \nsum \ksum r_{nk} \cdot \frac{a_k}{b_k} \cdot \E \left( \sum_{d=1}^D \beta_{kd} x_{nd} \right) \left( \sum_{j=1}^D \beta_{kj} x_{nj} \right) \\
	&= \nsum \ksum r_{nk} \cdot \frac{a_k}{b_k} \cdot \E\left( \sum_{d=1}^D (\beta_{kd}x_{nd})^2 + \sum_{d=1}^D \beta_{kd} x_{nd} \cdot \sum_{j\neq d}^D \beta_{kj}x_{nj}\right) \\
	&= \nsum \ksum r_{nk} \cdot \frac{a_k}{b_k} \cdot \E \left( \sum_{d=1}^D (\beta_{kd}x_{nd})^2 + \sum_{d=1}^D \sum_{j\neq d}^D \beta_{kd} \beta_{kj} x_{nd} x_{nj} \right) \\
	&= \sum_{d} \ksum \beta_{kd}^2 \cdot \frac{a_k}{b_k} \nsum r_{nk} x_{nd}^2 + \E \left( \sum_{d} \sum_{j\neq d} \ksum \beta_{kd} \Big[\frac{a_k}{b_k}\sum_n r_{nk} x_{nd} x_{nj} \Big] \beta_{kj} \right) \\
	&= \sum_d \transpose{\betad} \mathrm{U}_d \betad + \E \left( \sum_d \sum_{j\neq d} \transpose{\betad} \mathrm{R}_{dj} \tilde{\beta}_j \right) \\
	&= \sum_d \transpose{\betad} \mathrm{U}_d \betad + \sum_d \transpose{\betad} \sum_{j \neq d} \mathrm{R}_{dj} \E \big[ \tilde{\beta}_j \big]
\end{align*}

\begin{align*}
	\nsum \ksum r_{nk} \cdot \frac{a_k}{b_k} \cdot \E \left( y_n \transpose{\beta_k}x_n \right) &= 
	\nsum \ksum r_{nk} \cdot \frac{a_k}{b_k} \cdot y_n \cdot \E \left( \sum_{d=1}^D \beta_{kd} x_{nd}\right)\\
	&= \sum_d \sum_k \beta_{kd} \cdot \frac{a_k}{b_k} \cdot \sum_n y_n r_{nk} x_{nd} \\
	&= \sum_d \transpose{\betad} \zeta_d
\end{align*}


In order to obtain the update equation for $\lambda_d$, we first consider the terms in the variational lower bound that have $\lambda_d$ terms, denoted below as $F(\lambda_d)$. Equality is written up to constants in $\lambda_d$.

\begin{align*}
	F(\lambda_d) &= \E_q \ln \pr{\mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z}} + \E_q \ln \pr{\beta_d, \omega_d} - \E_q \ln q \left( \beta_d, \omega_d \right) \\
	&= - \frac{1}{2}\Big\{ \E_q \left( \transpose{\betad} \umat_d \betad\right) - 2 \transpose{\E_q \left( \betad \right)} \eta_d \Big\} \\
	& \quad + \E_q \Big\{ \omega_d \left( -\frac{K}{2} \ln (2\pi) + \frac{K}{2} \ln \xi_0 - \frac{\xi_0}{2} \transpose{\betad} \betad  \right) + \omega_d \ln \pi + (1 - \omega_d) \ln (1 - \pi) \Big\} \\
	& \quad - \E_q \Big\{ \frac{\omega_d}{2} \left( -K \ln (2 \pi) + \ln |\qmat_d | - \transpose{\left( \betad - m_d \right)} \qmat_d \left( \betad - m_d \right)\right) + \omega_d \ln \lambda_d + (1-\omega_d) \ln (1 - \lambda_d) \Big\} \\
	&= -\frac{\lambda_d }{2}  \cdot \Big\{ \trace{\umat_d \qmat_d^{-1}} + \transpose{m_d} \umat_d m_d - 2 \transpose{m_d} \eta_d \Big\}-\frac{\lambda_d }{2}  \cdot \Big\{ K \ln(2\pi) - K \ln \xi_0 \Big\} \\
	& \quad -\frac{\lambda_d }{2}  \cdot \Big\{ \xi_0 \cdot \trace{\qmat_d^{-1}} + \xi_0 \cdot \transpose{m_d} m_d \Big\} + \lambda_d \cdot \frac{K}{2} + \frac{\lambda_d}{2}  \cdot \Big\{ K \ln (2\pi) - \ln |\qmat_d | \Big\} + \nu_d \\
	&= \frac{\lambda_d}{2} \cdot \Bigg\{ K \ln \xi_0 - \ln |\qmat_d | + K -  \trace{\left(\umat_d + \xi_0 \cdot \mathrm{I}_K \right) \qmat_d^{-1}} - \transpose{m_d} \left( \umat_d + \xi_0 \cdot \mathrm{I}_K \right) m_d + 2\transpose{m_d}\eta_d \Bigg\} + \nu_d\\
	&= \frac{\lambda_d}{2} \cdot \Bigg\{ K \ln \xi_0 - \ln |\qmat_d | + K -  \trace{\qmat_d \qmat_d^{-1}} - \transpose{m_d} \qmat_d m_d + 2\transpose{m_d}\eta_d \Bigg\} + \nu_d\\ 
	&= \frac{\lambda_d}{2} \cdot \Bigg\{ K \ln \xi_0 - \ln |\qmat_d | + K - K - \transpose{m_d} \qmat_d \qmat_d^{-1} \eta_d + 2\transpose{m_d}\eta_d \Bigg\} + \nu_d\\ 
	&= \frac{\lambda_d}{2} \cdot \Bigg\{ K \ln \xi_0 - \ln |\qmat_d | - \transpose{m_d} \eta_d + 2\transpose{m_d}\eta_d \Bigg\} + \nu_d\\ 
	&= \frac{\lambda_d}{2} \cdot \Bigg\{ K \ln \xi_0 - \ln |\qmat_d | + \transpose{m_d} \eta_d \Bigg\} + \nu_d
\end{align*}

Differentiating with respect to $\lambda_d$, we obtain the following expression:
\begin{align*}
	\frac{\partial F(\lambda_d)}{\partial \lambda_d} &= \ln \frac{\pi}{1-\pi} - \ln \frac{\lambda_d}{1-\lambda_d} + \ln \xi^{\frac{\xi_0}{2}} + \ln |\qmat_d|^{- \frac{1}{2}} + \frac{1}{2} \transpose{m_d} \eta_d
\end{align*}

Setting the derivative to 0, we obtain the following update:
\begin{align*}
	\frac{\lambda_d}{1- \lambda_d} = \frac{\pi}{1-\pi} \cdot \xi_0^{\frac{K}{2}} \cdot |\qmat|^{-\frac{1}{2}} \cdot \exp \Big\{ \frac{1}{2} \cdot \transpose{m_d} \eta_d\Big\}
\end{align*}

We have used the following definition to ease notation in the calculations, $\nu_d = \lambda_d \ln \pi + (1 - \lambda_d) \ln(1-\pi) - \lambda_d \ln \lambda_d - (1 - \lambda_d) \ln (1 - \lambda_d)$

\newpage

\section{ELBO Computation Details}

%%% expectation of prior distributions

% E[ln p (y | - )]
\begin{align*}
	\E_q \ln p\left( \mathbf{y} \given \mathbf{X}, \boldbeta, \boldtau, \mathbf{Z} \right) &= 
	\E_q \nsum \ksum z_{nk} \Big\{ - \frac{1}{2} \ln(2\pi) + \frac{1}{2} \ln \tau_k - \frac{\tau_k}{2}(y_n - \transpose{x_n} \beta_k)^2\Big\} \\
	&= \nsum \ksum r_{nk} \Big\{ - \frac{1}{2} \ln(2\pi) + \frac{1}{2} (\psi(a_k) - \psi(b_k)) - \frac{a_k}{2b_k} \cdot \E(y_n - \transpose{x_n} \beta_k)^2\Big\} \\
	&= - \frac{1}{2} \nsum \ksum r_{nk} \Big\{  \ln(2\pi) - (\psi(a_k) - \psi(b_k)) + \frac{a_k}{b_k} \cdot y_n^2 \Big\} \\ & \qquad -\frac{1}{2} \cdot \E_q \sum_d \transpose{\betad} \mathrm{U}_d \betad + \transpose{\betad} \Bigg[\sum_{j \neq d} \mathrm{R}_{dj} \tilde{\beta}_j \Bigg] - 2 \transpose{\betad} \zeta_d \\
	&= - \frac{1}{2} \nsum \ksum r_{nk} \Big\{  \ln(2\pi) - (\psi(a_k) - \psi(b_k)) + \frac{a_k}{b_k} \cdot y_n^2 \Big\} \\ & \qquad -\frac{1}{2} \cdot \sum_d \E_q \Big[ \transpose{\betad} \mathrm{U}_d \betad \Big] - 2 \cdot \transpose{\E\left(\betad\right)} \Bigg[ \zeta_d - \frac{1}{2} \sum_{j \neq d} \mathrm{R}_{dj} \E\left(\tilde{\beta}_j\right) \Bigg] \\
	&= - \frac{1}{2} \nsum \ksum r_{nk} \Big\{  \ln(2\pi) - (\psi(a_k) - \psi(b_k)) + \frac{a_k}{b_k} \cdot y_n^2 \Big\} \\ & \qquad - \frac{1}{2} \cdot \sum_d \lambda_d \left( \transpose{m_d} U_d m_d + \trace{\mathrm{U}_d \mathrm{Q}_d^{-1}} - 2\transpose{m_d} \Bigg[ \zeta_d - \frac{1}{2} \sum_{j\neq d} \lambda_j \mathrm{R}_{dj} m_j\Bigg] \right)
\end{align*}

% E[ln p (z | - )]
\begin{align*}
	\E_q \ln p\left( \mathbf{Z} \given \mathbf{X}, \boldgamma \right) &= \E_q \nsum \ksum z_{nk} \Big\{ \transpose{x_n} \gamma_k - \ln \sum_{j=1}^K e^{\transpose{x_n}\gamma_j}\Big\} = \nsum \ksum r_{nk} \Big\{ \transpose{x_n} \mu_k - \alpha_n - \varphi_n \Big\}
\end{align*}

\begin{align*}
	\E_q \ln p\left( \boldgamma \right) &= \E_q \ksum  \ln \Big\{ (2\pi)^{-\frac{D}{2}} \cdot e^{-\frac{1}{2} \transpose{\gamma_k}\gamma_k}\Big\} = -\frac{KD}{2} \ln(2\pi) - \frac{1}{2} \left( \transpose{\mu_k} \mu_k + \trace{\mathrm{V}_k^{-1}} \right)
\end{align*}

\begin{align*}
	\E_q \ln p\left( \boldtau \right) &= \E_q \ksum \ln \Big\{ \frac{b_0^{a_0}}{\Gamma(a_0)} e^{-b_0 \tau_k} \tau_k^{a_0 - 1} \Big\} \\
	&= \ksum a_0 \ln b_0 - \ln \Gamma(a_0) - b_0 \cdot \frac{a_k}{b_k} + (a_0 - 1) \left( \psi(a_k) - \psi(b_k) \right) \\
	&= K \left( a_0 \ln b_0 - \ln \Gamma(a_0) \right) - \ksum b_0 \cdot \frac{a_k}{b_k} + (a_0 - 1) \left( \psi(a_k) - \psi(b_k) \right)
\end{align*}


\begin{align*}
	\E_q \ln p\left( \boldbeta, \boldomega \right) &= \E_q \sum_d \omega_d \ln \Big\{ (2\pi)^{-\frac{K}{2}} e^{-\frac{\xi_0}{2} \transpose{\betad} \betad}\Big\} + \omega_d \ln \pi + (1 - \omega_d) \ln \left( 1 - \pi \right) \\
	&= \sum_d \lambda_d \ln \pi + (1 - \lambda_d) \ln (1 - \pi) + \E_q \Big[ \omega_d \left( - \frac{K}{2} \ln (2\pi) - \frac{K}{2} \cdot \xi_0 \right) - \frac{\xi_2}{2} \cdot \omega_d \transpose{\betad} \betad\Big] \\
	&= \sum_d \lambda_d \ln \pi + (1 - \lambda_d) \ln (1 - \pi) + \frac{\lambda_d}{2} \cdot \Big\{ K \cdot \xi_0 - K \cdot \ln (2 \pi) - \xi_0 \cdot \left( \trace{\qmat_d^{-1}} + \transpose{m_d}m_d \right) \Big\}
\end{align*}

%%% expectation of variational distributions

\begin{align*}
	\E_q \ln q\left( \boldbeta, \boldomega \right) &=  \E_q \sum_d \omega_d \cdot \ln \Big\{ -\frac{K}{2} \ln (2\pi) + \frac{1}{2} \ln |\qmat_d | - \frac{1}{2} \transpose{\left( \betad - m_d \right)} \qmat_d \left( \betad - m_d \right) \Big\} \\
	& \qquad \qquad  + \omega_d \ln \lambda_d + (1 - \omega_d) \ln (1 - \lambda_d) \\
	&= \sum_d \lambda_d \ln \lambda_d + (1 - \lambda_d) \ln (1-\lambda_d) + \frac{\lambda_d}{2} \Big\{ - K \ln (2\pi) + \ln |\qmat_d|\Big\} \\
	& \qquad \qquad  - \frac{1}{2} \cdot \E_q \Big[ \omega_d \transpose{\left( \betad - m_d \right)} \qmat_d \left( \betad - m_d \right) \Big] \\
	&= \sum_d \lambda_d \ln \lambda_d + (1 - \lambda_d) \ln (1-\lambda_d) + \frac{\lambda_d}{2} \Big\{ - K \ln (2\pi) + \ln |\qmat_d| - K\Big\}
\end{align*}

\begin{align*}
	\E_q \ln q\left( \boldgamma \right) &= \E_q \ksum \ln \Big\{ (2\pi)^{-\frac{D}{2}} |\mathrm{V}_k|^{\frac{1}{2}} \cdot e^{-\frac{1}{2} \transpose{(\gamma_k	- \mu_k)} \mathrm{V}_k (\gamma_k - \mu_k)}\Big\}\\
	&= \ksum -\frac{D}{2} \ln (2\pi) + \frac{1}{2} \ln |\mathrm{V}_k|  -\frac{1}{2} \E_q \Big[ \transpose{(\gamma_k - \mu_k)} \mathrm{V}_k (\gamma_k- \mu_k)\Big] \\
	&= -\frac{KD}{2} \ln (2\pi) + \frac{1}{2} \cdot \ksum \ln \mathrm{V}_k - D \\
	&= - \frac{KD}{2} \left( \ln(2\pi) + 1 \right) + \frac{1}{2} \cdot \ksum \mathrm{V}_k
\end{align*}

\begin{align*}
	\E_q \ln q\left( \mathbf{Z} \right) &= \E_q \nsum \ksum z_{nk} \ln r_{nk} = \nsum \ksum r_{nk} \ln r_{nk}
\end{align*}

\begin{align*}
	\E_q \ln q\left( \boldtau \right) &= \E_q \ksum \ln \Bigg\{ \frac{b_k^{a_k}}{\Gamma(a_k)} \cdot  e^{-b_k \tau_k} \cdot  \tau_k^{a_k - 1}\Bigg\}\\
	&= \ksum a_k \left( \ln b_k - 1 \right) - \ln \Gamma(a_k)  + (a_k - 1) \cdot \left( \psi(a_k) - \psi(b_k) \right)
\end{align*}



\vskip 0.2in
\printbibliography

\end{document}